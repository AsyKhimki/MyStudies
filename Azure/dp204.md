<h1>Azure Data Engineer Associate </h1>

<h2> 1. Get started with data engineering on Azure </h2>

| Unit  | Topic   | Section       | Notes  |
| :---    | :---   | :---     | :---     |
| Introduction to data engineering on Azure | [Introduction](https://learn.microsoft.com/en-gb/training/modules/introduction-to-data-engineering-azure/1-introduction?ns-enrollment-type=learningpath&ns-enrollment-id=learn.wwl.get-started-data-engineering)| | - data engineer is the primary role responsible for integrating, transforming, and consolidating data from various structured and unstructured data systems into structures that are suitable for building analytics solutions|
| | [What is data engineering](https://learn.microsoft.com/en-gb/training/modules/introduction-to-data-engineering-azure/2-what-data-engineering)|- Types of data </br> - Data operations </br> - Data integration </br> - Data transformation </br> - Data consolidation </br> -  Common languages| - PDF is unstructured data </br> - **Data integration**: establishing links between operational and analytical services and data sources to enable secure, reliable access to data across multiple systems </br> - **Data transformation**: transform operational data (ETL or ELT) to into suitable structure and format for analysis </br> - **Data consolidation**: the process of combining data that has been extracted from multiple data sources into a consistent structure - usually to support analytics and reporting|
| | [Important data engineering concepts](https://learn.microsoft.com/en-gb/training/modules/introduction-to-data-engineering-azure/4-common-patterns-azure-data-engineering)| - Operational and analytical data </br> - Streaming data  </br> - Data pipelines </br> - Data lakes </br> - Data warehouses </br> - Apache Spark| - **Operational data**: transactional data that is generated and stored by application </br> - **Analytical data**: data that has been optimized for analysis and reporting </br></br> - **Streaming data**: perpetual sources of data that generate data values in real-time, often relating to specific events. </br> - **Data pipelines**: orchestrate activities that transfer and transform data. </br> - **Data lake**: a storage repository that holds large amounts of data in native, raw formats. </br> - **Data warehouses**: store current and historical data in relational tables that are organized into a schema that optimizes performance for analytical queries. </br> - **Apache Spark**: parallel processing framework that takes advantage of in-memory processing and a distributed file storage|
|| [Data engineering in Microsoft Azure](https://learn.microsoft.com/en-gb/training/modules/introduction-to-data-engineering-azure/5-common-tooling-azure-data-engineering) || - Streaming data is captured in event broker services such as **Azure Event Hubs** </br></br> The core Azure technologies used to implement data engineering workloads include: </br> - Azure Synapse Analytics </br> - Azure Data Lake Storage Gen2</br> - Azure Stream Analytics</br> - Azure Data Factory </br> - Azure Databricks|
| Introduction to Azure Data Lake Storage Gen2 | [Introduction](https://learn.microsoft.com/en-gb/training/modules/introduction-to-azure-data-lake-storage/1-introduction)| | Data lake  </br> - provides file-based storage, usually in a distributed file system that supports high scalability for massive volumes of data </br> - can store structured, semi-structured, and unstructured files |
||[Understand Azure Data Lake Storage Gen2](https://learn.microsoft.com/en-gb/training/modules/introduction-to-azure-data-lake-storage/2-azure-data-lake-gen2)| - Benefits </br> - Hadoop compatible access </br> - Security </br> - Performance </br> - Data redundancy | **Data lake** </br> - a repository of data that is stored in its natural format, usually as blobs or files. </br> - combines a file system with a storage platform </br> </br> Benefits </br> -  designed to deal with this variety and volume of data at exabyte scale while securely handling hundreds of gigabytes of throughput </br> - can treat the data as if it's stored in a Hadoop Distributed File System (HDFS) </br> - can store the data in one place and access it through compute technologies including Azure Databricks, Azure HDInsight, and Azure Synapse Analytics  </br> - supports access control lists (ACLs) and Portable Operating System Interface (POSIX) permissions </br> -  can set permissions at a directory level or file level </br> - All data that is stored is encrypted at rest </br> - organizes the stored data into a hierarchy of directories and subdirectories, much like a file system => less resources needed for processing </br> - takes advantage of the Azure Blob replication models|
||[Enable Azure Data Lake Storage Gen2 in Azure Storage](https://learn.microsoft.com/en-gb/training/modules/introduction-to-azure-data-lake-storage/3-create-data-lake-account)||- a configurable capability of a StorageV2 (General Purpose V2) Azure Storage </br> - select the option to **Enable hierarchical** namespace in the Advanced|
||[Compare Azure Data Lake Store to Azure Blob storage](https://learn.microsoft.com/en-gb/training/modules/introduction-to-azure-data-lake-storage/4-azure-data-lake-and-blob-storage)||Azure Blob </br> - an store large amounts of unstructured ("object") data in a flat namespace within a blob container </br> - the blobs are stored as a single-level hierarchy in a flat namespace </br> - can access this data by using HTTP or HTTPs </br> - can use to archive rarely used data or to store website assets such as images and media </br></br> Azure Data Lake Storage Gen2  </br> - builds on blob storage and optimizes I/O of high-volume data by using a hierarchical namespace</br> - metadata: operations, such as directory renames and deletes, to be performed in a single atomic operation </br> - better storage and retrieval performance for an analytical use case and lowers the cost of analysis.|
||[Understand the stages for processing big data](https://learn.microsoft.com/en-gb/training/modules/introduction-to-azure-data-lake-storage/5-stages-for-processing-big-data)|| four stages for processing big data solutions </br> - Ingest </br> - Store </br> - Prep and train </br> - Model and serve |
||[Use Azure Data Lake Storage Gen2 in data analytics workloads](https://learn.microsoft.com/en-gb/training/modules/introduction-to-azure-data-lake-storage/6-use-cases) | </br> - Big data processing and analytics  </br> - Data warehousing </br> - Real-time data analytics</br> -  Data science and machine learning| - enabling technology for multiple data analytics use cases. </br> - Often, the data is staged in a data lake in order to facilitate distributed processing before being loaded into a relational data warehouse </br> - **data lakehouse** or **lake database**: data warehouse uses external tables to define a relational metadata layer over files in the data lake => can then support analytical queries for reporting and visualization </br> - Azure Data Lake Storage Gen 2 provides a highly scalable cloud-based data store for the volumes of data required in data science workloads.|
| Introduction to Azure Synapse Analytics | [Introduction](https://learn.microsoft.com/en-gb/training/modules/introduction-azure-synapse-analytics/1-introduction)||- Azure Synapse Analytics provides a single, cloud-scale platform that supports multiple analytical technologies; enabling a consolidated and integrated experience for data engineers, data analysts, data scientists, and other professionals who need to work with data.|
||[What is Azure Synapse Analytics](https://learn.microsoft.com/en-gb/training/modules/introduction-azure-synapse-analytics/2-what-happening-business)||Common types of analytical techniques (Gartner) </br> - **Descriptive analytics** - “What is happening in my business?” </br> - **Diagnostic analytics** - “Why is it happening?” </br> - Predictive analytics </br> - **Prescriptive analytics**: enable autonomous decision making based on real-time or near real-time analysis of data, using predictive analytics. |
|| [How Azure Synapse Analytics works](https://learn.microsoft.com/en-gb/training/modules/introduction-azure-synapse-analytics/3-how-works) |- Creating and using an Azure Synapse Analytics workspace </br> - Working with files in a data lake </br> - Ingesting and transforming data with pipelines </br> - Querying and manipulating data with SQL </br> - Processing and analyzing data with Apache Spark </br> - Exploring data with Data Explorer </br> - Integrating with other Azure data services| - A Synapse Analytics workspace defines an instance of the Synapse Analytics service in which can manage the services and data resources needed for your analytics solution|

<h2> 4. Transfer and transform data with Azure Synapse Analytics pipelines </h2>

| Unit  | Topic   | Section       | Notes  |
| :---    | :---   | :---     | :---     |
| Build a data pipeline in Azure Synapse Analytics  |[Introduction](https://learn.microsoft.com/en-gb/training/modules/build-data-pipeline-azure-synapse-analytics/1-introduction) | |- **Pipelines** are a mechanism for defining and orchestrating data movement activities. </br> - The authoring processes described in this module are also applicable to **Azure Data Factory** </br> - [Data integration in Azure Synapse Analytics versus Azure Data Factory](https://learn.microsoft.com/en-us/azure/synapse-analytics/data-integration/concepts-data-factory-differences)| 
||[Understand pipelines in Azure Synapse Analytics](https://learn.microsoft.com/en-gb/training/modules/build-data-pipeline-azure-synapse-analytics/2-understand-pipelines-azure-synapse-analytics)|- Core pipeline concepts </br> - Activities </br> - Integration runtime </br> - Linked services </br> - Datasets|- can use a pipeline to define data transfer and transformation activities, and orchestrate these activities through control flow activities that manage branching, looping, and other typical processing logic </br></br> Activities </br> - executable tasks in a pipeline </br> - can encapsulate data transfer operations, including simple data copy operations that extract data from a source and load it to a target (or sink), as well as more complex data flows (ETL) </br> - **control flow** activities: use to implement loops, conditional branching</br></br> Integration runtime </br> - compute resources and an execution context in which to run</br></br> Linked services </br> - some activities depend on other services:  pipeline might include an activity to run a notebook in Azure Databricks or to call a stored procedure in Azure SQL Database|
||[Create a pipeline in Azure Synapse Studio](https://learn.microsoft.com/en-gb/training/modules/build-data-pipeline-azure-synapse-analytics/3-create-pipeline-azure-synapse-studio)|Defining a pipeline with JSON|Resources </br> - **UI**: set up pipeline</br> - **JSON**: pipeline definition </br> </br> - the primary place where pipelines are created and managed is the **Integrate** page. </br> - Succeeded, Failed, and Completed dependency conditions |
||[Define data flows](https://learn.microsoft.com/en-gb/training/modules/build-data-pipeline-azure-synapse-analytics/4-define-data-flows)||- A **Data Flow** is a commonly used activity type to define data flow and transformation. </br> - An important part of creating a data flow is to define mappings for the columns as the data flows through the various stages </br></br>Data flows consist of:</br>- Sources: The input data to be transferred. </br>- Transformations: Various operations that you can apply to data as it streams through the data flow. </br>- Sinks: Targets into which the data will be loaded.|
||[Run a pipeline](https://learn.microsoft.com/en-gb/training/modules/build-data-pipeline-azure-synapse-analytics/5-run-pipelines)||- can publish a pipeline and use a trigger to run it. </br></br> Pipeline triggers </br> - Immediately  </br> - At explicitly scheduled intervals  </br> - In response to an event, such as new data files being added to a folder in a data lake. </br> - integration with **Microsoft Purview**:  use pipeline run history to track data lineage data flows.|
||[Exercise - Build a data pipeline in Azure Synapse Analytics](https://learn.microsoft.com/en-gb/training/modules/build-data-pipeline-azure-synapse-analytics/6-exercise-build-data-pipeline-azure-synapse-analytics)||
|Use Spark Notebooks in an Azure Synapse Pipeline|[Introduction](https://learn.microsoft.com/en-gb/training/modules/use-spark-notebooks-azure-synapse-pipeline/1-introduction)||The Synapse Notebook activity enables you to run data processing code in Spark notebooks as a task in a pipeline|
||[Understand Synapse Notebooks and Pipelines](https://learn.microsoft.com/en-gb/training/modules/use-spark-notebooks-azure-synapse-pipeline/2-understand-notebooks-pipelines)||- can use external processing resources to perform specific tasks </br> - example: Apache Spark pool in Azure Synapse Analytics workspace</br> - The notebook is run on a Spark pool, which you can configure with the appropriate compute resources and Spark runtime </br> - The pipeline itself is run in an integration runtime that orchestrates the activities in the pipeline, coordinating the external services needed to run them </br> - **best practices**|
||[Use a Synapse notebook activity in a pipeline](https://learn.microsoft.com/en-gb/training/modules/use-spark-notebooks-azure-synapse-pipeline/3-use-notebook-activity)||- must add a notebook activity and configure it appropriately </br></br> Notebook activity specific settings </br> - The notebook you want to run </br> - Apache Spark pool on which the notebook should be run </br> - The node size for the worker nodes in the pool, </br>- Dynamically allocate executors </br> - The node size for the **driver node** </br> - Min / Max executors|
||[Use parameters in a notebook](https://learn.microsoft.com/en-gb/training/modules/use-spark-notebooks-azure-synapse-pipeline/4-notebook-parameters)|- Create a parameters cell in the notebook </br> - Set base parameters for the notebook activity| - parameters cell: toggle option in the notebook editor interface. </br> - expand and edit the Base parameters section of the settings for the activity </br> - Can assign explicit parameter values, or use an expression to assign a dynamic value|
||[Exercise - Use an Apache Spark notebook in a pipeline](https://learn.microsoft.com/en-gb/training/modules/use-spark-notebooks-azure-synapse-pipeline/5-exercise-use-spark-notebooks-pipeline)||

<h2>Work with Hybrid Transactional and Analytical Processing Solutions using Azure Synapse Analytics</h2>

| Unit  | Topic   | Section       | Notes  |
| :---    | :---   | :---     | :---     |
