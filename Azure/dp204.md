<h1>Azure Data Engineer Associate </h1>

<h2> 1. Get started with data engineering on Azure </h2>

| Unit  | Topic   | Section       | Notes  |
| :---    | :---   | :---     | :---     |
| Introduction to data engineering on Azure | [Introduction](https://learn.microsoft.com/en-gb/training/modules/introduction-to-data-engineering-azure/1-introduction?ns-enrollment-type=learningpath&ns-enrollment-id=learn.wwl.get-started-data-engineering)| | - data engineer is the primary role responsible for integrating, transforming, and consolidating data from various structured and unstructured data systems into structures that are suitable for building analytics solutions|
| | [What is data engineering](https://learn.microsoft.com/en-gb/training/modules/introduction-to-data-engineering-azure/2-what-data-engineering)|- Types of data </br> - Data operations </br> - Data integration </br> - Data transformation </br> - Data consolidation </br> -  Common languages| - PDF is unstructured data </br> - **Data integration**: establishing links between operational and analytical services and data sources to enable secure, reliable access to data across multiple systems </br> - **Data transformation**: transform operational data (ETL or ELT) to into suitable structure and format for analysis </br> - **Data consolidation**: the process of combining data that has been extracted from multiple data sources into a consistent structure - usually to support analytics and reporting|
| | [Important data engineering concepts](https://learn.microsoft.com/en-gb/training/modules/introduction-to-data-engineering-azure/4-common-patterns-azure-data-engineering)| - Operational and analytical data </br> - Streaming data  </br> - Data pipelines </br> - Data lakes </br> - Data warehouses </br> - Apache Spark| - **Operational data**: transactional data that is generated and stored by application </br> - **Analytical data**: data that has been optimized for analysis and reporting </br></br> - **Streaming data**: perpetual sources of data that generate data values in real-time, often relating to specific events. </br> - **Data pipelines**: orchestrate activities that transfer and transform data. </br> - **Data lake**: a storage repository that holds large amounts of data in native, raw formats. </br> - **Data warehouses**: store current and historical data in relational tables that are organized into a schema that optimizes performance for analytical queries. </br> - **Apache Spark**: parallel processing framework that takes advantage of in-memory processing and a distributed file storage|
|| [Data engineering in Microsoft Azure](https://learn.microsoft.com/en-gb/training/modules/introduction-to-data-engineering-azure/5-common-tooling-azure-data-engineering) || - Streaming data is captured in event broker services such as **Azure Event Hubs** </br></br> The core Azure technologies used to implement data engineering workloads include: </br> - Azure Synapse Analytics </br> - Azure Data Lake Storage Gen2</br> - Azure Stream Analytics</br> - Azure Data Factory </br> - Azure Databricks|
| Introduction to Azure Data Lake Storage Gen2 | [Introduction](https://learn.microsoft.com/en-gb/training/modules/introduction-to-azure-data-lake-storage/1-introduction)| | Data lake  </br> - provides file-based storage, usually in a distributed file system that supports high scalability for massive volumes of data </br> - can store structured, semi-structured, and unstructured files |
||[Understand Azure Data Lake Storage Gen2](https://learn.microsoft.com/en-gb/training/modules/introduction-to-azure-data-lake-storage/2-azure-data-lake-gen2)| - Benefits </br> - Hadoop compatible access </br> - Security </br> - Performance </br> - Data redundancy | **Data lake** </br> - a repository of data that is stored in its natural format, usually as blobs or files. </br> - combines a file system with a storage platform </br> </br> Benefits </br> -  designed to deal with this variety and volume of data at exabyte scale while securely handling hundreds of gigabytes of throughput </br> - can treat the data as if it's stored in a Hadoop Distributed File System (HDFS) </br> - can store the data in one place and access it through compute technologies including Azure Databricks, Azure HDInsight, and Azure Synapse Analytics  </br> - supports access control lists (ACLs) and Portable Operating System Interface (POSIX) permissions </br> -  can set permissions at a directory level or file level </br> - All data that is stored is encrypted at rest </br> - organizes the stored data into a hierarchy of directories and subdirectories, much like a file system => less resources needed for processing </br> - takes advantage of the Azure Blob replication models|
||[Enable Azure Data Lake Storage Gen2 in Azure Storage](https://learn.microsoft.com/en-gb/training/modules/introduction-to-azure-data-lake-storage/3-create-data-lake-account)||- a configurable capability of a StorageV2 (General Purpose V2) Azure Storage </br> - select the option to **Enable hierarchical** namespace in the Advanced|
||[Compare Azure Data Lake Store to Azure Blob storage](https://learn.microsoft.com/en-gb/training/modules/introduction-to-azure-data-lake-storage/4-azure-data-lake-and-blob-storage)||Azure Blob </br> - an store large amounts of unstructured ("object") data in a flat namespace within a blob container </br> - the blobs are stored as a single-level hierarchy in a flat namespace </br> - can access this data by using HTTP or HTTPs </br> - can use to archive rarely used data or to store website assets such as images and media </br></br> Azure Data Lake Storage Gen2  </br> - builds on blob storage and optimizes I/O of high-volume data by using a hierarchical namespace</br> - metadata: operations, such as directory renames and deletes, to be performed in a single atomic operation </br> - better storage and retrieval performance for an analytical use case and lowers the cost of analysis.|
||[Understand the stages for processing big data](https://learn.microsoft.com/en-gb/training/modules/introduction-to-azure-data-lake-storage/5-stages-for-processing-big-data)|| four stages for processing big data solutions </br> - Ingest </br> - Store </br> - Prep and train </br> - Model and serve |
||[Use Azure Data Lake Storage Gen2 in data analytics workloads](https://learn.microsoft.com/en-gb/training/modules/introduction-to-azure-data-lake-storage/6-use-cases) | </br> - Big data processing and analytics  </br> - Data warehousing </br> - Real-time data analytics</br> -  Data science and machine learning| - enabling technology for multiple data analytics use cases. </br> - Often, the data is staged in a data lake in order to facilitate distributed processing before being loaded into a relational data warehouse </br> - **data lakehouse** or **lake database**: data warehouse uses external tables to define a relational metadata layer over files in the data lake => can then support analytical queries for reporting and visualization </br> - Azure Data Lake Storage Gen 2 provides a highly scalable cloud-based data store for the volumes of data required in data science workloads.|
| Introduction to Azure Synapse Analytics | [Introduction](https://learn.microsoft.com/en-gb/training/modules/introduction-azure-synapse-analytics/1-introduction)||- Azure Synapse Analytics provides a single, cloud-scale platform that supports multiple analytical technologies; enabling a consolidated and integrated experience for data engineers, data analysts, data scientists, and other professionals who need to work with data.|
||[What is Azure Synapse Analytics](https://learn.microsoft.com/en-gb/training/modules/introduction-azure-synapse-analytics/2-what-happening-business)||Common types of analytical techniques (Gartner) </br> - **Descriptive analytics** - “What is happening in my business?” </br> - **Diagnostic analytics** - “Why is it happening?” </br> - Predictive analytics </br> - **Prescriptive analytics**: enable autonomous decision making based on real-time or near real-time analysis of data, using predictive analytics. |
|| [How Azure Synapse Analytics works](https://learn.microsoft.com/en-gb/training/modules/introduction-azure-synapse-analytics/3-how-works) |- Creating and using an Azure Synapse Analytics workspace </br> - Working with files in a data lake </br> - Ingesting and transforming data with pipelines </br> - Querying and manipulating data with SQL </br> - Processing and analyzing data with Apache Spark </br> - Exploring data with Data Explorer </br> - Integrating with other Azure data services| - A Synapse Analytics workspace defines an instance of the Synapse Analytics service in which can manage the services and data resources needed for your analytics solution|

<h2> 2. Build data analytics solutions using Azure Synapse serverless SQL pools </h2>

| Unit  | Topic   | Section       | Notes  |
| :---    | :---   | :---     | :---     |
| Use Azure Synapse serverless SQL pool to query files in a data lake | [Introduction](https://learn.microsoft.com/en-gb/training/modules/query-data-lake-using-azure-synapse-serverless-sql-pools/1-introduction)| | - SQL pools: tailored for querying data in a data lake </br> - can use SQL code to query data in files of various common formats without needing to load the file data into database storage. </br> - analyze and process file data in the data lake using a familiar data processing language, without the need to create or maintain a relational database store.| 
| |[Understand Azure Synapse serverless SQL pool capabilities and use cases](https://learn.microsoft.com/en-gb/training/modules/query-data-lake-using-azure-synapse-serverless-sql-pools/2-understand-serverless-pools) | - Serverless SQL pools in Azure Synapse Analytics </br> - When to use serverless SQL pools| - Azure Synapse Analytics is an integrated analytics service that brings together a wide range of commonly used technologies for processing and analyzing data at scale </br> - Azure Synapse SQL: a distributed query system in Azure Synapse Analytics </br></br> Runtime environments </br> - Serverless SQL pool: on-demand SQL query processing => pay-per-query endpoint to query </br> - Dedicated SQL pool: Enterprise-scale relational database instances used to host data warehouses </br></br> Benefits of serverless SQL pools </br> - A familiar Transact-SQL syntax to query data in place without the need to copy or load data </br> - Integrated connectivity from a wide range of business intelligence and ad-hoc querying tools </br> - Distributed query processing that is built for large-scale data </br> - Built-in query execution fault-tolerance, resulting in high reliability and success rates </br> - No infrastructure to setup or clusters to maintain </br> - No charge for resources reserved </br></br> When to use </br> - helps when need to know exact cost for each query executed to monitor and attribute costs </br> - not recommended for OLTP workloads such as databases </br></br> Common use cases </br> - Data exploration </br> - Data transformation </br> - Logical data warehouse (logical view)|
||[Query files using a serverless SQL pool](https://learn.microsoft.com/en-gb/training/modules/query-data-lake-using-azure-synapse-serverless-sql-pools/3-query-files)|- Querying delimited text files </br> - Specifying the rowset schema </br> - Querying JSON files </br> - Querying Parquet files </br> - Query partitioned data </br> - |- an use a serverless SQL pool to query data files in various common file formats </br> - The basic syntax for querying is the same for all of these types of file </br> - The output from OPENROWSET is a rowset to which an alias must be assigned </br> - The BULK parameter includes the full URL to the location in the data lake containing the data files </br> - can use wildcards in the BULK parameter to include or exclude files in the query </br> - OPENROWSET has no specific format for JSON files, so you must use csv format with FIELDTERMINATOR, FIELDQUOTE, and ROWTERMINATOR set to 0x0b </br></br> Code </br> - examples how to include / exclude files using BULK parameter </br> - query to re-write default column names </br> - return product data from a folder containing multiple JSON files </br> - To extract individual values from the JSON, you can use the JSON_VALUE function in the SELECT statement </br> - query parquet files </br> -  query that filters the results to include only the orders for January and February 2020|
||[Create external database objects](https://learn.microsoft.com/en-gb/training/modules/query-data-lake-using-azure-synapse-serverless-sql-pools/4-external-objects)|- Creating a database</br> - Creating an external data source </br> - Creating an external file format </br> -  </br> - Creating an external table | - create a custom database that contains some objects that make it easier to work with external data in the data lake that you need to query frequently - can create a database in a serverless SQL pool just as you would in a SQL Server instance </br> - One consideration is to set the collation of your database so that it supports conversion of text data in files to appropriate Transact-SQL data types </br> - if you plan to query data in the same location frequently, it's more efficient to define an external data source that references that location </br> - benefit of using a data source is that you can assign a credential for the data source to use when accessing the underlying storage, enabling you to provide access to data through SQL without permitting users to access the data directly in the storage account. </br> - format details for the file being access; which may include multiple settings for delimited text filesYou can encapsulate these settings in an external file format </br> - To simplify access to the data, you can encapsulate the files in an external table; which users and reporting applications can query using a standard SQL SELECT statement just like any other database table </br> - By creating a database that contains the external objects discussed in this unit, you can provide a relational database layer over files in a data lake </br></br> Code </br> - creates a database named salesDB with a collation that makes it easier to import UTF-8 encoded text data into VARCHAR columns. </br> - creates a data source named files for the hypothetical https://mydatalake.blob.core.windows.net/data/files/ folder|
||[Exercise - Query files using a serverless SQL pool](https://learn.microsoft.com/en-gb/training/modules/query-data-lake-using-azure-synapse-serverless-sql-pools/5-exercise-sql)|||
|Use Azure Synapse serverless SQL pools to transform data in a data lake|[Introduction](https://learn.microsoft.com/en-gb/training/modules/use-azure-synapse-serverless-sql-pools-for-transforming-data-lake/1-introduction)||- data engineers often need to use SQL to transform data; often as part of a data ingestion pipeline or extract, transform, and load (ETL) process </br> - CREATE EXTERNAL TABLE AS SELECT (CETAS): in a dedicated SQL pool or serverless SQL pool to persist the results of a query in an external table, which stores its data in a file in the data lake.|
||[Transform data files with the CREATE EXTERNAL TABLE AS SELECT statement](https://learn.microsoft.com/en-gb/training/modules/use-azure-synapse-serverless-sql-pools-for-transforming-data-lake/2-transform-data-using-create-external-table-select-statement)|- Creating external database objects to support CETAS </br> - External data source </br> - External file format </br> - Using the CETAS statement </br> - Dropping external tables|- Azure Synapse serverless SQL pools can be used to run SQL statements that transform data and persist the results as a file in a data lake for further processing or querying </br> - an external table: a metadata object in a database that provides a relational abstraction over data stored in files. </br></br> To use CETAS expressions, you must create the following types of object  </br> - External data source: encapsulates a connection to a file system location in a data lake. </br> - can encapsulate a credential in the external data source so that it can be used to access file data without granting all users permissions to read it directly </br> - External file format: pecify the format of the files you want to create as an external file format.</br> - [external file formats](https://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-file-format-transact-sql?view=sql-server-ver16&tabs=delimited) </br> - After creating an external data source and external file format, you can use the CETAS statement to transform data and stored the results in an external table. </br> - you must use an external data source to specify the location where the transformed data for the external table is to be saved. </br> - external tables are a metadata abstraction over the files that contain the actual data. Dropping an external table does not delete the underlying files. </br></br> Code </br> - transform coma-delimited text file into parquet </br> - drop external from the database |
||[Encapsulate data transformations in a stored procedure](https://learn.microsoft.com/en-gb/training/modules/use-azure-synapse-serverless-sql-pools-for-transforming-data-lake/3-operationalize-data-transformation-using-stored-procedures)||- it's good practice to encapsulate the transformation operation in stored procedure. </br></br> Benefits </br> - Reduces client to server network traffic: The commands in a procedure are executed as a single batch of code </br> - Provides a security boundary: The procedure controls what processes and activities are performed and protects the underlying database objects </br> - Eases maintenance </br> - Improved performance: Stored procedures are compiled the first time they're executed, and the resulting execution plan is held in the cache </br></br> Code </br> - code to create procedures|
||[Include a data transformation stored procedure in a pipeline](https://learn.microsoft.com/en-gb/training/modules/use-azure-synapse-serverless-sql-pools-for-transforming-data-lake/4-pool-stored-procedures-synapse-pipelines)||- In Azure Synapse Analytics and Azure Data Factory, you can create pipelines that connect to linked services, including Azure Data Lake Store Gen2 storage accounts that host data lake files, and serverless SQL pools; enabling you to call your stored procedures as part of an overall data extract, transform, and load (ETL) pipeline </br> - Creating a pipeline for the data transformation enables you to schedule the operation to run at specific times or based on specific events|
||[Exercise - Transform files using a serverless SQL pool](https://learn.microsoft.com/en-gb/training/modules/use-azure-synapse-serverless-sql-pools-for-transforming-data-lake/4a-exercise-transform-data)|||
|Create a lake database in Azure Synapse Analytics|[Introduction](https://learn.microsoft.com/en-gb/training/modules/create-metadata-objects-azure-synapse-serverless-sql-pools/1-introduction)||- choose between the flexibility of storing data files in a data lake, with the advantages of a structured schema in a relational database </br> - Lake databases in Azure Synapse Analytics provide a way to combine these two approaches and benefit from an explicit relational schema of tables, views, and relationships that is decoupled from file-based storage.|
||[Understand lake database concepts](https://learn.microsoft.com/en-gb/training/modules/create-metadata-objects-azure-synapse-serverless-sql-pools/2-lake-database)|- Lake database schema </br> - Lake database storage </br> - Lake database compute |- In a data lake, there is no fixed schema. </br> - A lake database provides a relational metadata layer over one or more files in a data lake. </br> - the storage of the data files is decoupled from the database schema; enabling more flexibility than a relational database system typically offers. </br> - The data for the tables in your lake database is stored in the data lake as Parquet or CSV files. </br> - To query and manipulate the data through the tables you have defined, you can use an Azure Synapse serverless SQL pool to run SQL queries or an Azure Synapse Apache Spark pool|
||[Explore database templates](https://learn.microsoft.com/en-gb/training/modules/create-metadata-objects-azure-synapse-serverless-sql-pools/3-database-templates)||- can create a Lake database from an empty schema, to which you add definitions for tables and the relationships between them </br> - Azure Synapse Analytics provides a comprehensive collection of database templates that reflect common schemas found in multiple business scenarios|
||[Create a lake database](https://learn.microsoft.com/en-gb/training/modules/create-metadata-objects-azure-synapse-serverless-sql-pools/4-create-lake-database)|- use lake database designer in Azure Synapse Studio </br> - it's advisable to store all of the database files in a consistent format within the same root folder in the data lake.|
||[Use a lake database](https://learn.microsoft.com/en-gb/training/modules/create-metadata-objects-azure-synapse-serverless-sql-pools/5-use-lake-database)| - After creating a lake database, you can store data files that match the table schemas in the appropriate folders in the data lake, and query them using SQL.|
||[Exercise - Analyze data in a lake database](https://learn.microsoft.com/en-gb/training/modules/create-metadata-objects-azure-synapse-serverless-sql-pools/6-exercise-lake-database)||
| Secure data and manage users in Azure Synapse serverless SQL pools |[Choose an authentication method in Azure Synapse serverless SQL pools](https://learn.microsoft.com/en-gb/training/modules/secure-data-manage-users-azure-synapse-serverless-sql-pools/2-choose-authentication-method)|- Authorization </br> - Access to storage accounts|- authentication: to how users prove their identity when connecting to the endpoint </br> - two types of authentication supported: </br> - SQL Authentication: user name and password </br> - Microsoft Entra authentication: identities managed by Entra id. Use Active Directory authentication (integrated security) whenever possible. </br> - Authorization: what can the user do => controlled by your user account's database role memberships and object-level permissions. </br> - If Microsoft Entra authentication is used, a user can sign in to a serverless SQL pool and other services, like Azure Storage, and can grant permissions to the Microsoft Entra user. </br> </br> Serverless SQL pool supports the following authorization types </br> - Anonymous access </br> - Shared access signature (SAS): delegated access - can grant clients access to resources in storage account, without sharing account keys. </br> - User Identity Also known as "pass-through", is an authorization type where the identity of the Microsoft Entra user that logged into serverless SQL pool is used to authorize access to the data.|
||[Manage users in Azure Synapse serverless SQL pools](https://learn.microsoft.com/en-gb/training/modules/secure-data-manage-users-azure-synapse-serverless-sql-pools/3-manage-users)||- give administrator privileges to a user to Azure Synapse serverless SQL pool. |
||[Manage user permissions in Azure Synapse serverless SQL pools](https://learn.microsoft.com/en-gb/training/modules/secure-data-manage-users-azure-synapse-serverless-sql-pools/4-manage-user-permissions)||- Azure Storage supports: Azure role-based access control (Azure RBAC) and access control lists (ACLs) like Portable Operating System Interface for Unix (POSIX)|

 </br> - can query a lake database in a SQL script by using a serverless SQL pool.|

<h2> 3. Transfer and transform data with Azure Synapse Analytics pipelines </h2>

| Unit  | Topic   | Section       | Notes  |
| :---    | :---   | :---     | :---     |


<h2> 4. Transfer and transform data with Azure Synapse Analytics pipelines </h2>

| Unit  | Topic   | Section       | Notes  |
| :---    | :---   | :---     | :---     |
| Build a data pipeline in Azure Synapse Analytics  |[Introduction](https://learn.microsoft.com/en-gb/training/modules/build-data-pipeline-azure-synapse-analytics/1-introduction) | |- **Pipelines** are a mechanism for defining and orchestrating data movement activities. </br> - The authoring processes described in this module are also applicable to **Azure Data Factory** </br> - [Data integration in Azure Synapse Analytics versus Azure Data Factory](https://learn.microsoft.com/en-us/azure/synapse-analytics/data-integration/concepts-data-factory-differences)| 
||[Understand pipelines in Azure Synapse Analytics](https://learn.microsoft.com/en-gb/training/modules/build-data-pipeline-azure-synapse-analytics/2-understand-pipelines-azure-synapse-analytics)|- Core pipeline concepts </br> - Activities </br> - Integration runtime </br> - Linked services </br> - Datasets|- can use a pipeline to define data transfer and transformation activities, and orchestrate these activities through control flow activities that manage branching, looping, and other typical processing logic </br></br> Activities </br> - executable tasks in a pipeline </br> - can encapsulate data transfer operations, including simple data copy operations that extract data from a source and load it to a target (or sink), as well as more complex data flows (ETL) </br> - **control flow** activities: use to implement loops, conditional branching</br></br> Integration runtime </br> - compute resources and an execution context in which to run</br></br> Linked services </br> - some activities depend on other services:  pipeline might include an activity to run a notebook in Azure Databricks or to call a stored procedure in Azure SQL Database|
||[Create a pipeline in Azure Synapse Studio](https://learn.microsoft.com/en-gb/training/modules/build-data-pipeline-azure-synapse-analytics/3-create-pipeline-azure-synapse-studio)|Defining a pipeline with JSON|Resources </br> - **UI**: set up pipeline</br> - **JSON**: pipeline definition </br> </br> - the primary place where pipelines are created and managed is the **Integrate** page. </br> - Succeeded, Failed, and Completed dependency conditions |
||[Define data flows](https://learn.microsoft.com/en-gb/training/modules/build-data-pipeline-azure-synapse-analytics/4-define-data-flows)||- A **Data Flow** is a commonly used activity type to define data flow and transformation. </br> - An important part of creating a data flow is to define mappings for the columns as the data flows through the various stages </br></br>Data flows consist of:</br>- Sources: The input data to be transferred. </br>- Transformations: Various operations that you can apply to data as it streams through the data flow. </br>- Sinks: Targets into which the data will be loaded.|
||[Run a pipeline](https://learn.microsoft.com/en-gb/training/modules/build-data-pipeline-azure-synapse-analytics/5-run-pipelines)||- can publish a pipeline and use a trigger to run it. </br></br> Pipeline triggers </br> - Immediately  </br> - At explicitly scheduled intervals  </br> - In response to an event, such as new data files being added to a folder in a data lake. </br> - integration with **Microsoft Purview**:  use pipeline run history to track data lineage data flows.|
||[Exercise - Build a data pipeline in Azure Synapse Analytics](https://learn.microsoft.com/en-gb/training/modules/build-data-pipeline-azure-synapse-analytics/6-exercise-build-data-pipeline-azure-synapse-analytics)||
|Use Spark Notebooks in an Azure Synapse Pipeline|[Introduction](https://learn.microsoft.com/en-gb/training/modules/use-spark-notebooks-azure-synapse-pipeline/1-introduction)||The Synapse Notebook activity enables you to run data processing code in Spark notebooks as a task in a pipeline|
||[Understand Synapse Notebooks and Pipelines](https://learn.microsoft.com/en-gb/training/modules/use-spark-notebooks-azure-synapse-pipeline/2-understand-notebooks-pipelines)||- can use external processing resources to perform specific tasks </br> - example: Apache Spark pool in Azure Synapse Analytics workspace</br> - The notebook is run on a Spark pool, which you can configure with the appropriate compute resources and Spark runtime </br> - The pipeline itself is run in an integration runtime that orchestrates the activities in the pipeline, coordinating the external services needed to run them </br> - **best practices**|
||[Use a Synapse notebook activity in a pipeline](https://learn.microsoft.com/en-gb/training/modules/use-spark-notebooks-azure-synapse-pipeline/3-use-notebook-activity)||- must add a notebook activity and configure it appropriately </br></br> Notebook activity specific settings </br> - The notebook you want to run </br> - Apache Spark pool on which the notebook should be run </br> - The node size for the worker nodes in the pool, </br>- Dynamically allocate executors </br> - The node size for the **driver node** </br> - Min / Max executors|
||[Use parameters in a notebook](https://learn.microsoft.com/en-gb/training/modules/use-spark-notebooks-azure-synapse-pipeline/4-notebook-parameters)|- Create a parameters cell in the notebook </br> - Set base parameters for the notebook activity| - parameters cell: toggle option in the notebook editor interface. </br> - expand and edit the Base parameters section of the settings for the activity </br> - Can assign explicit parameter values, or use an expression to assign a dynamic value|
||[Exercise - Use an Apache Spark notebook in a pipeline](https://learn.microsoft.com/en-gb/training/modules/use-spark-notebooks-azure-synapse-pipeline/5-exercise-use-spark-notebooks-pipeline)||

<h2>Work with Hybrid Transactional and Analytical Processing Solutions using Azure Synapse Analytics</h2>

| Unit  | Topic   | Section       | Notes  |
| :---    | :---   | :---     | :---     |
