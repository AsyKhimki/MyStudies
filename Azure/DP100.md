<h1> Microsoft Certified: Azure Data Scientist Associate </h1>

**Reference**
1. [Study Guide](https://learn.microsoft.com/en-gb/credentials/certifications/resources/study-guides/dp-100)

<h2> 1. Design a machine learning solution </h2>

| Unit  | Topic   | Comments       |
| :---    | :---   | :---     |
| Design a data ingestion strategy for machine learning projects | [Identify your data source and format](https://learn.microsoft.com/en-gb/training/modules/design-data-ingestion-strategy-for-machine-learning-projects/2-identify-your-data-source-format) |  |
| | [Choose how to serve data to machine learning workflows](https://learn.microsoft.com/en-gb/training/modules/design-data-ingestion-strategy-for-machine-learning-projects/3-choose-how-serve-data-workflows) | - Storing data separately from your compute - minimize costs and be more flexible </br> - Which tool or service is best to store the data depends on the data you have and the service you use for model training.| 
| | [Design a data ingestion solution](https://learn.microsoft.com/en-gb/training/modules/design-data-ingestion-strategy-for-machine-learning-projects/4-solution) | - Data ingestion pipeline: a sequence of tasks that move and transform the data. </br> - Azure Synapse Pipelines: UI or JSON + SQL, Python, or R. </br> - Azure Synapse Compute: Serverless SQL pools, dedicated SQL pools, or Spark pools </br> - Azure Databricks: Code first, in notebooks (sql, R, Python) uses Spark clusters </br> - Azure ML: compute clusters, which automatically scale up and down when needed </br> - Pipeline: Designer, or by creating a collection of scripts </br> - Azure Synapse Analytics and Azure Databricks - more scalable </br> - All tasks within the same tool - Azure ML|
| | [Exercise: Design a data ingestion strategy](https://learn.microsoft.com/en-gb/training/modules/design-data-ingestion-strategy-for-machine-learning-projects/5-exercise) ||
| Design a machine learning model training solution | [Introduction](https://learn.microsoft.com/en-gb/training/modules/design-machine-learning-model-training-solution/1-introduction)||
|| [Identify machine learning tasks](https://learn.microsoft.com/en-gb/training/modules/design-machine-learning-model-training-solution/2-identify-tasks) | - See the list of common machine learning tasks |
|| [Choose a service to train a machine learning model](https://learn.microsoft.com/en-gb/training/modules/design-machine-learning-model-training-solution/3-choose-service-train) | - Which service you use depends on factors like: </br> - What are the differences|
|| [Decide between compute options](https://learn.microsoft.com/en-gb/training/modules/design-machine-learning-model-training-solution/4-decide-between-compute-options) | - should monitor compute utilization to know when to scale up or down to save on time and costs. </br> - CPU or GPU </br> - For smaller tabular datasets, CPU will be sufficient and cheaper to use </br> - Unscructured - GPU </br> - When processing your data and training your model takes a long time => potentially GPU </br> - General purpose or memory optimized Spark </br> - Spark compute or clusters use the same sizing as virtual machines in Azure but distribute the workloads. </br> - size of compute in Azure Machine Learning is shown as the virtual machine size. The sizes follow the same naming conventions as Azure Virtual Machines. </br> - Monitor the compute utilization </br> - how long it takes to train the model and how much compute is used to execute your code. </br> - If training too long - maybe GPU? / Spark|
|| [Exercise: Design a model training strategy](https://learn.microsoft.com/en-gb/training/modules/design-machine-learning-model-training-solution/5-exercise) ||
| Design a model deployment solution | [Understand how model will be consumed](https://learn.microsoft.com/en-gb/training/modules/design-model-deployment-solution/2-understand-how-model-consumed)| - plan how you integrate the model, as it may affect how you train the model or what training data you use. </br> -  integrate the model, you need to deploy a model to an endpoint. </br> - an endpoint can be a web address that an application can call to get a message back.</br> - Endpoint options: real-time predictions, batch predictions </br> -  Real-time: score any new data as it comes in </br> - Batch: want the model to score new data in batches, and save the results as a file or in a database| 
|| [Decide on real-time or batch deployment](https://learn.microsoft.com/en-gb/training/modules/design-model-deployment-solution/3-decide-real-time-batch-deployment)|- frequency of scoring: how often and how quickly you need the predictions to be generated. </br> - model's predictions are only consumed at certain times - batch "number of predictions" </br> - cost of compute </br> - real-time or batch endpoint, you'll use different types of compute </br> - real-time predictions, you need compute that is always available and able to return the results (almost) immediately.  => Azure Container Instance (ACI) and Azure Kubernetes Service (AKS) </br> - continuously paying for the compute </br> -  batch predictions, you need compute that can handle a large workload </br> - By letting the workspace scale down an idle compute cluster, you can save significant costs. </br> - There are scenarios where you expect to need real-time predictions when batch predictions can be more cost-effective. </br> - Simpler models require less cost and time to generate predictions.|
|Design a machine learning operations solution| [Explore an MLOps architecture](https://learn.microsoft.com/en-gb/training/modules/design-machine-learning-operations-solution/2-explore-machine-learning-operations-solution-architecture)| - prepare the model and operationalize it, </br> - environment refers to a collection of resources </br> - Organize Azure Machine Learning environments </br> - dev, pre-prod, and prod environment </br> - Having separate environments makes it easier to control access to resources. Each environment can then be </br> - associated with a separate Azure Machine Learning workspace. </br> - use one workspace for development and production, you have a smaller Azure footprint and less management overhead => cant separate access Design an MLOps architecture|
| | [Design for monitoring](https://learn.microsoft.com/en-gb/training/modules/design-machine-learning-operations-solution/3-design-monitoring)| - you'll want to monitor the model, the data, and the infrastructure to collect metrics that help you decide on any necessary next steps. </br> -  MLflow to train and track your machine learning models. </br> - monitor for any responsible artificial intelligence (AI) issues </br> - which performance metrics you want to monitor and what the benchmark for each metric should be </br> - Monitor the data </br> - historical data: there may be trends that change the profile of the data, making your model less accurate. </br> - change in data profiles between current and the training data is known as data drift </br> - retrain models as required to maintain predictive accuracy. </br> -  Monitor the infrastructure  </br> - "minimize cost and optimize performance." "By reviewing compute utilization, you know whether you can scale down your provisioned compute, or whether you need to scale out to avoid capacity constraints."|
| | [Design for retraining](https://learn.microsoft.com/en-gb/training/modules/design-machine-learning-operations-solution/4-design-retraining) | - Two approaches to design for retraining </br> - Schedule </br> - Metric </br> - Pre-requisite: prepare code for automation </br> - scripts instead of notebooks with parameters </br> - host the code in a central repository </br> - Automate your code => onfigure Azure Machine Learning </br> - Can trigger a job from another tool: Azure DevOps and GitHub (Actions), ML CLI extention|

<h2> 2. Explore and configure the Azure Machine Learning workspace </h2>

| Unit  | Topic   | Comments       |
| :---    | :---   | :---     |
| Design a data ingestion strategy for machine learning projects | table | example |

<h2> 3. Experiment with Azure Machine Learning </h2>

| Unit  | Topic   | Comments       |
| :---    | :---   | :---     |
| Find the best classification model with Automated Machine Learning | [Introduction](https://learn.microsoft.com/en-gb/training/modules/find-best-classification-model-automated-machine-learning/1-introduction) | - AutoML allows you to try multiple preprocessing transformations and algorithms with your data to find the best machine learning model. </br> - AutoML experiment using the visual interface of Azure Machine Learning studio, the Azure command-line interface (CLI), or the Python software development kit (SDK).|
|| [Preprocess data and configure featurization](https://learn.microsoft.com/en-gb/training/modules/find-best-classification-model-automated-machine-learning/2-preprocess-data-configure-featurization)|- 1. create a data asset in Azure Machine Learning </br> - 2. create a MLTable data asset that includes the schema of the data </br> -  3. AutoML applies scaling and normalization to numeric data automatically </br> - Apply featurization, can be customized </br> -  get notified if AutoML has detected any issues with the data, like whether there are missing values or class imbalance.|
|| [Run an Automated Machine Learning experiment](https://learn.microsoft.com/en-gb/training/modules/find-best-classification-model-automated-machine-learning/3-run-job)| - The algorithms AutoML uses will depend on the task you specify. </br> - List of algorithms for classification </br> - You can choose to block individual algorithms from being selected (compliance or save time) </br> -  primary metric is the target performance metric for which the optimal model will be determined. </br> - Set the limits </br> - Set the training properties </br> - hen you use a compute cluster, you can have as many parallel trials as you have nodes </br> -  can exclude (or include) a subset of the available algorithms. </br> - an also choose whether you want to allow AutoML to use ensemble models. </br> - Code to submit and monitor|
||[Evaluate and compare models](https://learn.microsoft.com/en-gb/training/modules/find-best-classification-model-automated-machine-learning/4-evaluate-compare-models)| - data guardrails will automatically be applied too. list of guardrails for classification </br> - To explore a model even further, you can generate explanations for each model that has been trained. |
|| [Exercise - Find the best classification model with Automated Machine Learning](https://learn.microsoft.com/en-gb/training/modules/find-best-classification-model-automated-machine-learning/5-exercise)||
| Track model training in Jupyter notebooks with MLflow | [Configure MLflow for model tracking in notebooks](	https://learn.microsoft.com/en-gb/training/modules/track-model-training-jupyter-notebooks-mlflow/2-use-mlflow-model-tracking) | - MLflow is an open-source library for tracking and managing your machine learning experiments. </br> - MLflow Tracking is a component of MLflow that logs everything about the model you're training, such as parameters, metrics, and artifacts. </br> - Set up on Azure + locally | 
| | [Train and track models in notebooks](https://learn.microsoft.com/en-gb/training/modules/track-model-training-jupyter-notebooks-mlflow/3-train-models-notebooks) | - To group model training results, you'll use experiments. </br> -  Log results with MLflow </br> - Enable autologging: supports automatic logging for popular machine learning libraries </br> - When the job has completed, you can review all logged metrics in the studio. </br> - Use custom logging: log supplementary or custom material that isn't logged through autologging. </br> - Common functions used with custom logging are:|
|| [Exercise - Track model training](https://learn.microsoft.com/en-gb/training/modules/track-model-training-jupyter-notebooks-mlflow/4-exercise-track-model-training)||


<h2> 4. Optimize model training with Azure Machine Learning </h2>

| Unit  | Topic   | Comments       |
| :---    | :---   | :---     |
| Run a training script as a command job in Azure Machine Learning | [Convert a notebook to a script](https://learn.microsoft.com/en-gb/training/modules/run-training-script-command-job-azure-machine-learning/2-convert-notebook-script) | - recommendations when creating scripts to have production-ready code </br> - Remove nonessential code.</br> - Refactor your code into functions.</br> - Test your script in the terminal.</br> - How to navigate to the terminal? |
||[Run a script as a command job](https://learn.microsoft.com/en-gb/training/modules/run-training-script-command-job-azure-machine-learning/3-run-script-command-job)|- What parameters are needed to configure </br> - How the jobs are grupped|
||[Use parameters in a command job](https://learn.microsoft.com/en-gb/training/modules/run-training-script-command-job-azure-machine-learning/4-use-parameters-command-job)|- pass a parameter value to a script you want to run as a command job|
||[Exercise - Run a training script as a command job](https://learn.microsoft.com/en-gb/training/modules/run-training-script-command-job-azure-machine-learning/5-exercise-run-training-script-command-job)||
|Track model training with MLflow in jobs| [Introduction](https://learn.microsoft.com/en-gb/training/modules/train-models-training-mlflow-jobs/1-introduction)| - MLflow is an open-source platform that helps you to track model metrics and artifacts across platforms and is integrated with Azure Machine Learning </br> - can run training scripts locally or in the cloud and compare the results|
||[Track metrics with MLflow](https://learn.microsoft.com/en-gb/training/modules/train-models-training-mlflow-jobs/2-track-metrics-mlflow)|- Understand MLflow </br> -  two options to track machine learning jobs with MLflow: Before you can use either of these options, you need to set up the environment to use MLflow. </br> - Include MLflow in the environment</br> - Enable autologging</br> - Autologging logs parameters, metrics, and model artifacts without anyone needing to specify what needs to be logged.</br> - Log metrics with MLflow</br> - "Depending on the type of value you want to log, use the MLflow command to store the metric with the experiment run:"</br> - Submit the job|
| | [View metrics and evaluate models](https://learn.microsoft.com/en-gb/training/modules/train-models-training-mlflow-jobs/3-view-metrics-evaluate-models)| - View the metrics in the Azure Machine Learning studio</br> -  Retrieve runs and metrics with MLflow.</br> -  you can query the runs in a notebook by using MLflow</br> -  Retrieve runs (in a notebook)|
||[Exercise - Use MLflow to track training jobs](https://learn.microsoft.com/en-gb/training/modules/train-models-training-mlflow-jobs/4-exercise-use-mlflow-track-training-jobs)|- When a data scientist enables MLflow autologging, where can all model assets be found? |


<h2> 5. Manage and review models in Azure Machine Learning </h2>

| Unit  | Topic   | Comments       |
| :---    | :---   | :---     |
| Design a data ingestion strategy for machine learning projects | table | example |


<h2> 6. Deploy and consume models with Azure Machine Learning </h2>

| Unit  | Topic   | Comments       |
| :---    | :---   | :---     |
| Deploy a model to a managed online endpoint| [Intro](https://learn.microsoft.com/en-gb/training/modules/deploy-model-managed-online-endpoint/1-introduction) | - To consume the model, you need to deploy it </br> - One way to deploy a model is to integrate it with a service that allows applications to request instant, or real-time, predictions for individual or small sets of data points|
| | [Explore managed online endpoints](https://learn.microsoft.com/en-gb/training/modules/deploy-model-managed-online-endpoint/2-explore-managed-online-endpoints)| - Real-time predictions </br> - An endpoint is an HTTPS endpoint to which you can send data, and which will return a response (almost) immediately. </br> - Azure Machine Learning, there are two types of online endpoints </br> - Managed online endpoints: Azure Machine Learning manages all the underlying infrastructure. </br> - Kubernetes online endpoints: Users manage the Kubernetes cluster which provides the necessary infrastructure. </br> - a managed online endpoint, you only need to specify the virtual machine (VM) type and scaling setting </br> - Deploy your model 4 things to specify to deploy your model to a managed online endpoint </br> - Blue/green deployment </br> - One endpoint can have multiple deployments. </br> - 90% of the traffic can go to the blue deployment*, and 10% of the traffic can go to the green deployment </br> - Create an endpoint|
||[Deploy your MLflow model to a managed online endpoint](https://learn.microsoft.com/en-gb/training/modules/deploy-model-managed-online-endpoint/3-deploy-your-mlflow-model-managed-online-endpoint)| - The easiest way to deploy a model to an online endpoint is to use an MLflow model and deploy it to a managed online endpoint. </br> - Deploy an MLflow model to an endpoint </br> - "don´t need to have the scoring script and environment." </br> - must have model files stored on a local path or with a registered model </br> - you also need to specify the compute configuration for the deployment|
|| [Deploy a model to a managed online endpoint](https://learn.microsoft.com/en-gb/training/modules/deploy-model-managed-online-endpoint/4-eploy-custom-model-managed-online-endpoint)|- "need to create the scoring script and define the environment necessary during inferencing.</br> - Deploy a model to an endpoint </br> - Pre-requisits</br> - Create the scoring script</br> - Create an environment</br> - Create the deployment</br> - can specify the compute configuration with two parameters (instance type, instance count)|
||[Test managed online endpoints](https://learn.microsoft.com/en-gb/training/modules/deploy-model-managed-online-endpoint/5-monitor-online-endpoints)| - Use the Azure Machine Learning studio</br> - List all endpointx</br> - Use the Azure Machine Learning Python SDK</br> - The response from the deployed model is a JSON collection with a prediction for each case that was submitted in the data.|
|| [Exercise - Deploy an MLflow model to an online endpoint](https://learn.microsoft.com/en-gb/training/modules/deploy-model-managed-online-endpoint/6-exercise-deploy-mlflow-model-online-endpoint)| - You should use Kubernetes online endpoint if you want to manage the underlying Kubernetes clusters.|
|Deploy a model to a batch endpoint| [Introduction](https://learn.microsoft.com/en-gb/training/modules/deploy-model-batch-endpoint/1-introduction) |- in machine learning, batch inferencing is used to asynchronously apply a predictive model to multiple cases and write the results to a file or database.|
|| [Understand and create batch endpoints](https://learn.microsoft.com/en-gb/training/modules/deploy-model-batch-endpoint/2-explore-batch-endpoints)|- Batch predictions</br> - An endpoint is an HTTPS endpoint that you can call to trigger a batch scoring job.</br> - can trigger the batch scoring job from another service, such as Azure Synapse Analytics or Azure Databricks.  => can integrate with existing pipeline</br> - uses a compute cluster to score multiple inputs</br> - Create a batch endpoint</br> - Use compute clusters for batch deployments</br> - The ideal compute to use for batch deployments is the Azure Machine Learning compute cluster|
|| [Deploy your MLflow model to a batch endpoint](https://learn.microsoft.com/en-gb/training/modules/deploy-model-batch-endpoint/3-deploy-your-mlflow-model-batch-endpoint)| - Register an MLflow model </br> - To avoid needed a scoring script and environment, an MLflow model needs to be registered in the Azure Machine Learning workspace </br> - Deploy an MLflow model to an endpoint</br> - need to specify how you want the batch scoring job to behave|
||[Deploy a custom model to a batch endpoint](https://learn.microsoft.com/en-gb/training/modules/deploy-model-batch-endpoint/4-deploy-custom-model-batch-endpoint)| - Create the scoring script: that reads the new data, loads the model, and performs the scoring. </br> - must include two functions: init(), run() </br> - Create an environment: can create an environment with a Docker image with Conda dependencies, or with a Dockerfile. Configure and create the deployment|
|| [Invoke and troubleshoot endpoints](https://learn.microsoft.com/en-gb/training/modules/deploy-model-batch-endpoint/5-monitor-batch-endpoints)| - When you invoke a batch endpoint, you trigger an Azure Machine Learning pipeline job.</br> - Trigger the batch scoring job </br> - Troubleshoot a batch scoring job|
|| [Exercise - Deploy an MLflow model to a batch endpoint](https://learn.microsoft.com/en-gb/training/modules/deploy-model-batch-endpoint/6-exercise-deploy-mlflow-model-batch-endpoint)| - The default deployment will be used to do the actual batch scoring when the endpoint is invoked.|






