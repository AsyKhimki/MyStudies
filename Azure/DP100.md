<h1> Microsoft Certified: Azure Data Scientist Associate </h1>

**Reference**
1. [Study Guide](https://learn.microsoft.com/en-gb/credentials/certifications/resources/study-guides/dp-100)

<h2> 1. Design a machine learning solution </h2>

| Unit  | Topic   | Section       | Notes  |
| :---    | :---   | :---     | :---     |
| Design a data ingestion strategy for machine learning projects | [Identify your data source and format](https://learn.microsoft.com/en-gb/training/modules/design-data-ingestion-strategy-for-machine-learning-projects/2-identify-your-data-source-format) | - Identify the data source </br> - Identify the data format </br> - Identify the desired data format| Steps for Data Scientist </br> 1. **Define the problem**: Decide on what the model should predict and when it's successful. </br> 2. **Get the data**: Find data sources and get access. </br> 3. **Prepare the data**: Explore the data. Clean and transform the data based on the model's requirements. <br> 4. **Train the model**: Choose an algorithm and hyperparameter values based on trial and error. </br> 5. **Integrate the model**: Deploy the model to an endpoint to generate predictions. </br> 6. **Monitor the model**: Track the model's performance. </br> </br> - Before being able to design the ETL or ELT process, you’ll need to identify your data source and data format.</br></br> Data Formats </br> - Tabular or structured data </br> - Semi-structured data </br> - Unstructured data: can't query the data in the database
| | [Choose how to serve data to machine learning workflows](https://learn.microsoft.com/en-gb/training/modules/design-data-ingestion-strategy-for-machine-learning-projects/3-choose-how-serve-data-workflows) | - Separate compute from storage </br> - Store data for model training workloads | Separate compute from storage </br> - ability to scale compute up or down </br> - can shut down compute without loosing the data </br> => it’s a best practice to store your data in one tool, which is separate from another tool you use to train your models.  </br> </br> Tools to train ML </br> - Azure Machine Learning </br> - Azure Databricks </br> - Azure Synapse Analytics </br> </br> Tools to store the data </br> - Azure Blob Storage </br> - Azure Data Lake Storage (Gen 2) </br> - Azure SQL Database| 
| | [Design a data ingestion solution](https://learn.microsoft.com/en-gb/training/modules/design-data-ingestion-strategy-for-machine-learning-projects/4-solution) | - Create a data ingestion pipeline </br> - Azure Synapse Analytics </br> - Azure Databricks </br> - Azure Machine Learning </br> - Design a data ingestion solution | - **Data ingestion pipeline**: a sequence of tasks that move and transform the data. </br>  It's a best practice to think about the architecture of a data ingestion solution before training model. </br></br> **Azure Synapse Pipelines**</br> - Pipeline definition: UI or JSON </br> - Data transformation: UI tool like mapping data flow or SQL, Python, or R. </br> - Compute resources: serverless SQL pools, dedicated SQL pools, or Spark pools. **Azure Databricks** </br> - Pipeline: in notebooks using SQL, R, Python </br> - Compute: Spark clusters </br></br> **Azure Machine Learning** </br> - Pipeline: Designer, or creating a collection of scripts </br> - Compute clusters, which automatically scale up and down when needed </br> - Pipeline: Designer, or by creating a collection of scripts </br></br> **Comparison criteria** </br> - Azure Synapse Analytics and Azure Databricks - more scalable </br> - All tasks within the same tool - Azure ML|
| | [Exercise: Design a data ingestion strategy](https://learn.microsoft.com/en-gb/training/modules/design-data-ingestion-strategy-for-machine-learning-projects/5-exercise) ||
| Design a machine learning model training solution | [Introduction](https://learn.microsoft.com/en-gb/training/modules/design-machine-learning-model-training-solution/1-introduction)||
|| [Identify machine learning tasks](https://learn.microsoft.com/en-gb/training/modules/design-machine-learning-model-training-solution/2-identify-tasks) | | Starting questions </br> -What the model’s output should be. </br> - What type of machine learning task you’ll use. </br> - What criteria makes a model successful. </br></br> See the list of common machine learning tasks </br> - Classification: Predict a categorical value. </br> - Regression: Predict a numerical value.</br> - Time-series forecasting: Predict future numerical values based on time-series data.</br> - Computer vision: Classify images or detect objects in images.</br> - Natural language processing (NLP): Extract insights from text.|
|| [Choose a service to train a machine learning model](https://learn.microsoft.com/en-gb/training/modules/design-machine-learning-model-training-solution/3-choose-service-train) | - Understand the difference between services | Important factors </br> - type of model </br> - need full control over model training. </br>- How much time you want to invest in model training. </br> - Which services are already within organization. </br> Which programming language </br></br> Commonly used services to train machine learning </br> - Azure Machine Learning </br> -  Azure Databricks Azure </br> - Synapse Analytics Azure AI Services</br> </br> General guidelines</br> - one of the customizable prebuilt models suits your requirements =>  Azure AI => save time and effort. </br> - want to keep all data-related (data engineering and data science) projects within the same service => Azure Synapse Analytics or Azure Databricks  </br> - need distributed compute for working with large datasets => Azure Synapse Analytics or Azure Databricks </br> - full control over model training and management => Azure Machine Learning or Azure Databricks </br> - Python is your preferred programming language =>  Azure Machine Learning  </br> - an intuitive user interface to manage ML lifecycle => Azure ML |
|| [Decide between compute options](https://learn.microsoft.com/en-gb/training/modules/design-machine-learning-model-training-solution/4-decide-between-compute-options) | - CPU or GPU </br> - General purpose or memory optimized </br> - Spark </br> - Monitor the compute utilization|**CPU or GPU** </br> - smaller tabular datasets => CPU: sufficient and cheaper </br> - unstructured => GPU </br> - processing data and training model takes a long time => GPU? </br> </br> **General purpose or memory optimized** </br> -**General purpose**: balanced CPU-to-memory ratio. </br> - testing and development with smaller datasets.</br> -**Memory optimized**: high memory-to-CPU ratio. </br> -Great for in-memory analytics, when have larger datasets or when working in notebooks. </br> </br> Spark </br> - Spark compute or clusters use the same sizing as virtual machines in Azure but distribute the workloads. </br> - size of compute in Azure ML is shown as the virtual machine size. </br>- sizes follow the same naming conventions as Azure VMs. </br></br> Monitor the compute utilization </br> - how long it takes to train the model? </br> how much compute is used to execute your code. </br> - If training too long - maybe GPU? / Spark|
|| [Exercise: Design a model training strategy](https://learn.microsoft.com/en-gb/training/modules/design-machine-learning-model-training-solution/5-exercise) ||
| Design a model deployment solution | [Understand how model will be consumed](https://learn.microsoft.com/en-gb/training/modules/design-model-deployment-solution/2-understand-how-model-consumed)| - Deploy a model to an endpoint </br> - Get real-time predictions </br> - Get batch predictions | **Definitions** </br> - integrate the model: deploy a model to an endpoint </br> - **an endpoint** can be a web address that an application can call to get a message back.</br> </br> - plan how you integrate the model, as it may affect how you train the model or what training data you use. </br></br> Endpoint options: </br> -  **Real-time**: score any new data as it comes in </br> - **Batch**: want the model to score new data in batches, and save the results as a file or in a database| 
|| [Decide on real-time or batch deployment](https://learn.microsoft.com/en-gb/training/modules/design-model-deployment-solution/3-decide-real-time-batch-deployment)| - Identify the necessary frequency of scoring </br> - Decide on the number of predictions </br> - Consider the cost of compute </br> - Decide on real-time or batch deployment | - model's predictions are only consumed at certain times => batch "number of predictions" </br></br>  **compute resources (cost)** </br> **real-time predictions** </br> - need compute that is always available and able to return the results (almost) immediately. => Azure Container Instance (ACI) and Azure Kubernetes Service (AKS) </br> - continuously paying for the compute </br></br>  **batch predictions** </br> - need compute that can handle a large workload </br> - scale down => can save significant costs. </br></br> - There are scenarios where expect to need real-time predictions when batch predictions can be more cost-effective. </br> - Simpler models require less cost and time to generate predictions.|
|Design a machine learning operations solution| [Explore an MLOps architecture](https://learn.microsoft.com/en-gb/training/modules/design-machine-learning-operations-solution/2-explore-machine-learning-operations-solution-architecture)| - Set up environments for development and production </br> - Organize Azure Machine Learning environments </br> - Design an MLOps architecture | Steps prepare the model and operationalize it </br> - Convert the model training to a robust and reproducible pipeline. </br> - Test the code and the model in a development environment. </br> - Deploy the model in a production environment. </br> - Automate the end-to-end process. </br></br>**environment** </br> - a collection of resources </br> - dev, pre-prod, and prod environment </br> - having separate environments makes it easier to control access to resources. </br>- Each environment can then be associated with a separate Azure Machine Learning workspace. </br> - use one workspace for development and production, you have a smaller Azure footprint and less management overhead => cant separate access Design an MLOps architecture </br> </br> Components of sample MLOps architecture</br> 1. **Setup**: Create all necessary Azure resources for the solution. </br>2. **Model development (inner loop)**: Explore and process the data to train and evaluate the model. </br> 3. Continuous integration: Package and register the model. </br> 4. **Model deployment** (outer loop): Deploy the model. </br> 5. **Continuous deployment**: Test the model and promote to production environment. </br> 6. **Monitoring**: Monitor model and endpoint performance.|
| | [Design for monitoring](https://learn.microsoft.com/en-gb/training/modules/design-machine-learning-operations-solution/3-design-monitoring)| - Monitor the model </br> - Monior the data </br> - Monitor the infrastructure | **Monitor the model** </br> - use `MLflow` to train and track your machine learning models. </br> - monitor for any responsible artificial intelligence (AI) issues </br> - decide which performance metrics you want to monitor and what the benchmark for each metric should be </br></br> **Monitor the data** </br> - historical data: there may be trends that change the profile of the data, making your model less accurate. </br> - change in data profiles between current and the training data is known as **data drift** </br> - retrain models as required to maintain predictive accuracy. </br></br> **Monitor the infrastructure**  </br> - minimize cost and optimize performance. </br> - compute might be the highest cost </br> - by reviewing compute utilization, know whether can scale down provisioned compute, need to scale out to avoid capacity constraints.|
| | [Design for retraining](https://learn.microsoft.com/en-gb/training/modules/design-machine-learning-operations-solution/4-design-retraining) | - Prepare your code </br> - Automate your code | Options to retrain </br> - Schedule </br> - Metric </br></br> Pre-requisite: </br> - prepare code for automation </br> - scripts instead of notebooks with parameters </br> - host the code in a central repository </br></br> Automate your code </br>- configure Azure Machine Learning </br> - can trigger a job from another tool: Azure DevOps and GitHub (Actions), ML CLI extention|

<h2> 2. Explore and configure the Azure Machine Learning workspace </h2>

| Unit  | Topic   | Section    | Notes    |
| :---    | :---   | :---     | :---     |
| Explore Azure Machine Learning workspace resources and assets | [Create an Azure Machine Learning workspace](https://learn.microsoft.com/en-gb/training/modules/explore-azure-machine-learning-workspace-resources-assets/2-provision) | - Understand the Azure Machine Learning service </br> - Create the workspace </br> - Give access to the Azure Machine Learning workspace </br> - Organize your workspaces | - The workspace is central place where you can work with all resources, assets available to train and deploy machine learning models. </br> - stores a history of all training jobs, including logs, metrics, outputs, and a snapshot of your code.  </br>- Steps to create ML workspace </br></br> Azure will automatically create other Azure resources within the same resource group to support the workspace </br> - **Azure Storage Account**store files and notebooks used in the workspace, to store metadata of jobs and models. </br> - **Azure Key Vault**: securely manage secrets such as authentication keys and credentials used by the workspace. </br> - **Application Insights**: To monitor predictive services in the workspace. </br> - **Azure Container Registry**: Created when needed to store images for Azure Machine Learning environments. </br></br> How to create Azure workspace </br> - Azure portal </br> - ARM template </br> -  Azure Command Line Interface (CLI)</br> - Azure Machine Learning Python SDK </br></br> **RBAC** </br> - **Owner**: full access to all resources, can grant access to others using access control. </br> - **Contributor**: full access to all resources, but can't grant access to others. </br> - **Reader**: Can view the resource, but isn't allowed to make any changes. </br></br> Azure Machine Learning roles </br> - **AzureML Data Scientist**: perform all actions within the workspace, except for creating or deleting compute resources, or editing the workspace settings. </br> - **AzureML Compute operator**: allowed to create, change, and manage access the compute resources within a workspace. |
||[Identify Azure Machine Learning resources](https://learn.microsoft.com/en-gb/training/modules/explore-azure-machine-learning-workspace-resources-assets/3-identify-resources)|- Create and manage the workspace </br> - Create and manage compute resources </br> - Create and manage datastores |   - **workspace**: top-level resource for Azure ML => mind access </br></br> Types of compute resources</br> - **compute instances**: similiar to vms, managed by workspace, ideal as dev environment for notebooks </br> - **compute clusters** on-demand clusters of CPU or GPU,  ideal for prod workload, as they can scale </br> - **Kubernetes clusters**: can attach AKS,  ideal to deploy trained machine learning models in prod scenarios.</br> - **serverless compute**: good for training </br> - **attached computes**: attach other Azure compute resources to the workspace (Azure Databricks, Synapse Spark pools). </br></br> Datastores  </br> - references to Azure data services, connection info stored in keyvalut</br> </br> automatically created datastores </br> - `workspaceartifactstore`: => azureml => compute and experiment logs of running jobs</br> - `workspaceworkingdirectory`: =>  file share of the Azure Storage</br> - `workspaceblobstore:` => Blob Storage => default data source </br> - `workspacefilestore`: Connects to the file share of the Azure Storage </br> - can create datastores to connect to other Azure data services|
||[Identify Azure Machine Learning assets](https://learn.microsoft.com/en-gb/training/modules/explore-azure-machine-learning-workspace-resources-assets/4-identify-assets)| - Create and manage models </br> - Create and manage environments </br> - Create and manage data </br> - Create and manage components| Examples of ML Assets </br> - models </br> - environments </br> - data</br> - components</br></br> **Models** </br> - Formats: `.pkl`, MLModel format specify the name and version</br> **Compute** </br> - ensure that your code runs on any compute that is available to you (requirements.txt => env)</br>**Environments** </br> - specify software packages, environment variables, and software settings to run scripts.</br>  **Data assets** </br>  - a specific file or folder. </br> - can use data assets to easily access data every time, without having to provide authentication</br> - create a data asset: specify the path to point to the file or folder, the name, the version. </br> **Component** </br> - reuse snippets of code from other projects.</br> - need to specify: name, version, code, and environment</br> - can use components when creating pipelines|
||[Train models in the workspace](https://learn.microsoft.com/en-gb/training/modules/explore-azure-machine-learning-workspace-resources-assets/5-run-jobs)| - Explore algorithms and hyperparameter values with Automated Machine Learning </br> - Run a notebook </br> -Run a script as a job | Options to train models with the Azure Machine Learning workspace: </br> - Use Automated Machine Learning. </br> - Run a Jupyter notebook. </br> Run a script as a job. </br></br> Run a notebook</br> - all files you clone or create in the notebooks section are stored in the file share of the Azure Storage</br></br> Run a script as a job types of jobs depending on a workload</br> - **command**: cxecute a single script.</br> - **sweep**: perform hyperparameter tuning when executing a single script.</br> - **pipeline**: consist of multiple scripts or components.|
||[Exercise - Explore the workspace](https://learn.microsoft.com/en-gb/training/modules/explore-azure-machine-learning-workspace-resources-assets/6-exercise-explore-workspace)||
|Explore developer tools for workspace interaction|[Explore the studio](https://learn.microsoft.com/en-gb/training/modules/explore-developer-tools-for-workspace-interaction/2-explore-studio)| - Access the studio |- Azure ML studio is a web portal, which provides an overview of all resources and assets available in the workspace.</br> - Access the studio + screenshot</br></br> What you can do in the studio </br> - **Author**: Create new jobs to train </br> - **Assets**: Create and review assets you use when training </br> - **Manage**: Create and manage resources</br></br> Use case </br> - ideal for quick experimentation or when you want to explore your past jobs </br> - when a pipeline job has failed, you can use the studio to navigate to the logs and review the error messages. |
||[Explore the Python SDK](https://learn.microsoft.com/en-gb/training/modules/explore-developer-tools-for-workspace-interaction/3-explore-python-sdk)| - Install the Python SDK</br> - Connect to workspace</br> - Use the reference documentation| To authenticate, you need the values of </br> -`subscription_id`: Your subscription ID. </br> -`resource_group`: The name of your resource group. </br> -`workspace_name`: The name of your workspace. </br></br> - Reference documentation links|
||[Explore the CLI](https://learn.microsoft.com/en-gb/training/modules/explore-developer-tools-for-workspace-interaction/4-explore-cli)| - Install the Azure CLI </br> - Install the Azure Machine Learning extension </br> - Work with the Azure CLI | Advantages to using the Azure CLI with Azure Machine Learning</br> - Automation, Consistency, Integration with DevOps, CI/CD</br> - [Useful source:](https://learn.microsoft.com/en-us/training/paths/train-models-azure-machine-learning-cli-v2/) </br> - [Reference documentation](https://learn.microsoft.com/en-us/cli/azure/ml/compute?view=azure-cli-latest) </br> - can also create the same compute target by first defining the configuration in a YAML file </br> - [yml resource specification](https://learn.microsoft.com/en-us/azure/machine-learning/reference-yaml-compute-aml?view=azureml-api-2)|
||[Exercise - Explore the developer tools](https://learn.microsoft.com/en-gb/training/modules/explore-developer-tools-for-workspace-interaction/5-exercise)||
|Work with environments in Azure Machine Learning| [Introduction](https://learn.microsoft.com/en-gb/training/modules/work-environments-azure-machine-learning/1-introduction) | | - **environments** list and store the necessary packages that you can reuse across compute targets.|
||[Understand environments](https://learn.microsoft.com/en-gb/training/modules/work-environments-azure-machine-learning/2-understand-environments)| - What is an environment in Azure Machine Learning? | - when create an Azure ML workspace, **curated environments** are automatically created and made available</br> - **custom environments** possible to define consistent, reusable runtime contexts for experiments, regardless of where the experiment script is run</br></br> - Azure ML builds environment definitions into Docker images and conda environments </br> When use an environment, Azure ML builds the environment on the Azure Container registry associated with the workspace. </br> Python code snippets </br> - view all available environments within the Azure ML workspace </br> - view details of a specific environment|
||[Explore and use curated environments](https://learn.microsoft.com/en-gb/training/modules/work-environments-azure-machine-learning/3-explore-use-curated-environments)| - Use a curated environment </br> - Test and troubleshoot a curated environment |- **curated environments**: prebuilt environments for the most common machine learning workloads, available in workspace by default. </br> - use the prefix AzureML</br> - scripts that use popular ML toolings</br> - Most commonly, you use environments ro run a script as a (command) job. /br> - curated environments allow for faster deployment time </br> - review the detailed error logs in the **Outputs + logs** tab of your job </br></br> Code snippets </br> - configure a command job with the Python SDK, which uses a curated environment including Scikit-Learn </br> - retrieve the description and tags of a curated environment with the Python SDK |
||[Create and use custom environments](https://learn.microsoft.com/en-gb/training/modules/work-environments-azure-machine-learning/4-create-use-custom-environments)| - Create a custom environment from a Docker image </br> - Create a custom environment with a conda specification file </br> - Use an environment | - can define an environment from a Docker image, a Docker build context, and a conda specification with Docker image.</br></br> Create a custom environment with a conda specification file</br> - When you need to include other packages or libraries in your environment, you can add a conda specification file to a Docker image when creating the environment. </br> - When you submit the job, the environment is built. The first time you use an environment, it can take 10-15 minutes to build the environment.  </br></br>Code snippets </br> - create an environment from a Docker image </br> - use the Azure ML base images to create an environment </br> - create an environment from a base Docker image and a conda specification file </br> - configure a command job with the Python SDK, which uses a curated environment including Scikit-Learn: |
||[Exercise - Work with environments](https://learn.microsoft.com/en-gb/training/modules/work-environments-azure-machine-learning/6-knowledge-check)||
|  Make data available in Azure Machine Learning | [Introduction](https://learn.microsoft.com/en-gb/training/modules/make-data-available-azure-machine-learning/)| | Learning objectives </br> - Access data by using Uniform Resource Identifiers (URIs). </br>-Connect to cloud data sources with datastores. </br> - Use data asset to access specific files or folders.| 
||[Understand URIs](https://learn.microsoft.com/en-gb/training/modules/make-data-available-azure-machine-learning/2-understand-uris)|- Understand URIs| - To find and access data in Azure Machine Learning, you can use Uniform Resource Identifiers (URIs). </br> - A **URI** references the location of your data </br> </br> Common protocols when working with data in the context of Azure Machine Learning </br> - `http(s)`: data storespublicly or privately in an Azure Blob Storage or publicly available http(s) location</br> - `abfs(s)`: data stores in an Azure Data Lake Storage Gen 2. </br> - `azureml`: Use for data stored in a datastore </br></br> - whenever possible, you should work with datastores and data assets in Azure Machine Learning (not to reveal sensitive info)</br> - A datastore is a reference to an existing storage account on Azure (Blob or data lake) - When you refer to the datastore , you won't need to authenticate as the connection information stored with the datastore will be used by Azure ML|
||[Create a datastore](https://learn.microsoft.com/en-gb/training/modules/make-data-available-azure-machine-learning/3-create-datastore)| - Understand types of datastores </br> - Use the built-in datastores </br> - Create a datastore </br> - Create a datastore for an Azure Blob Storage container | - datastores are abstractions for cloud data sources. They encapsulate the information needed to connect to data sources, and securely store this connection information so that you don’t have to code it in your scripts. </br>-Datastores allow you to easily connect to storage services without having to provide all necessary details every time you want to read or write data. </br> </br> Benefits of using data stores </br> - Provides easy-to-use URIs to your data storage.</br> - Facilitates data discovery within Azure Machine Learning. </br> - Securely stores connection information, without exposing secrets and keys to data scientists.</br></br> Authentication methods </br> - Credential-based: service principal, shared access signature (SAS) token or account key </br> - Identity based Microsoft Entra identity or managed identity. </br></br> Types of data stores </br> - Azure Blob Storage </br> - Azure File Share </br> - Azure Data Lake (Gen 2)</br></br> - Every workspace has four built-in datastores (two connecting to Azure Storage blob containers, and two connecting to Azure Storage file shares) </br></br> can create a datastore through</br> - GUI </br> Azure CLI </br> - Python SDK. </br></br> Code snipet </br> - create a datastore to connect to an Azure Blob Storage container with an account key </br> - ... with a SAS token|
||[Create a data asset](https://learn.microsoft.com/en-gb/training/modules/make-data-available-azure-machine-learning/4-create-data-asset)| - Understand data assets </br> - When to use data assets </br> - Create a URI file data asset </br> - Create a MLTable data asset | - To simplify getting access to the data, use data assets. </br> - **data assets**: references to where the data is stored, how to get access, and any other relevant metadata </br></br> </br> - can share and reuse data with other members of the team, they don't need to remember file locations. </br> - can seamlessly access data during model training (on any supported compute type) without worrying about connection strings or data paths </br> - You can version the metadata of the data asset.</br></br> Three main types of data assets </br> - **URI file**: Points to a specific file./br> - **URI folder**: Points to a folder.</br> - **MLTable**: Points to a folder or file, and includes a schema to read as tabular data.</br></br> - Data assets are most useful when executing machine learning tasks as Azure Machine Learning jobs </br> - in the code need to read the data before using it </br> - use a MLTable data asset when the schema of your data is complex or changes frequently </br> - Instead of changing how to read the data in every script that uses the data, you only have to change it in the data asset itself </br> - For certain features in Azure Machine Learning, like Automated Machine Learning, you need to use a MLTable data asset </br> - To define the schema, you can include a `MLTable` file in the same folder as the data you want to read </br> </br> Code</br> - Create and read an URI file data asset </br> - Create and read a URI folder data asset with the Python SDK </br> - Create and read a MLTable data asset with the Python SDK|
| Work with compute targets in Azure Machine Learning | [Choose appropriate compute targets](https://learn.microsoft.com/en-gb/training/modules/work-compute-resources-azure-machine-learning/2-compute-targets) || - **compute targets**: physical or virtual computers on which jobs are run.</br></br> Types of Compute </br> **Compute instance**: </br> - similiar to VM </br> - primarily to run notebooks </br> - ideal for experimentation.</br> **Compute clusters**: </br> - clusters of VM that automatically scale up or down </br> allow you to use parallel processing to distribute the workload & reduce time</br> **Kubernetes clusters**: </br> - more control over how the compute is configured and managed. </br> **Attached compute** </br>  - attach existing compute (Azure VMs or Azure Databricks)</br> - **Serverless compute**: A fully managed, on-demand compute you can use for training jobs. </br></br> Compute target for experimentation </br> - A notebook experience benefits most from a compute that is continuously running => compute instance </br> - Spark serverless compute to run Spark code in notebooks, if you want to make use of Spark's distributed compute power. </br></br> Production </br>  - want the compute target to be ready to handle large volumes of data </br> - compute cluster automatically scales up and down </br> - an alternative that you don't have to create and manage, you can use Azure Machine Learning's serverless compute </br></br> Deployment </br> - batch or real-time predictions? </br> - batch: compute clusters and Azure Machine Learning's serverless compute are ideal for pipeline jobs as they're on-demand and scalable </br> - real time: type of compute that is running continuously </br> - benefit from more lightweight compute </br> - Containers & Kubernetes clusters to manage the necessary compute to generate real-time predictions.|
||[Create and use a compute instance](https://learn.microsoft.com/en-gb/training/modules/work-compute-resources-azure-machine-learning/3-create-use-compute-instance)| - Create a compute instance with the Python SDK</br> - Assign a compute instance to a user </br> - Minimize compute time </br> - Use a compute instance| Create a compute instance in </br> - Azure Machine Learning studio </br> - Azure CLI </br> - Python SDK </br></br> - [Reference documentation](https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.entities.computeinstance?view=azure-python) </br> - When you need to create compute instances for multiple users, using a script allows you to create a consistent development environment for everyone. </br> - oto work with the compute instance, it needs to be assigned to you as a user. </br> - each instance can be assigned to 1 user (an't handle parallel workloads.)</br> - stop compute instance to save costs </br> - When a compute instance is assigned to you, can start and stop </br> - can add a schedule to the compute instance to start or stop at set times </br> - can configure a compute to automatically shut down when it has been idle for a set amount of time </br> - Easiest option to work with the compute instance is through the integrated notebooks experience in the Azure ML studio.|
||[Create and use a compute cluster](https://learn.microsoft.com/en-gb/training/modules/work-compute-resources-azure-machine-learning/4-create-use-compute-cluster)| - Create a compute cluster with the Python SDK</br> - Use a compute cluster |- in prod better to use scripts than notebooks </br></br> three parameters to consider when creating a cluster </br> - `size`: Specifies the virtual machine type of each node within the compute  cluster. </br> - `max_instances`: Specifies the maximum number of nodes </br> `tier`: Specifies whether your virtual machines are low priority or dedicated. </br></br> - [sizes of VMs](https://learn.microsoft.com/en-us/azure/virtual-machines/sizes/overview?tabs=breakdownseries%2Cgeneralsizelist%2Ccomputesizelist%2Cmemorysizelist%2Cstoragesizelist%2Cgpusizelist%2Cfpgasizelist%2Chpcsizelist) </br> Main scenarios when to use compute cluster </br> - Running a pipeline job you built in the Designer. </br> -  Running an Automated Machine Learning job </br> - Running a script as a job. </br></br> - allows you to train multiple models in parallel|

<h2> 3. Experiment with Azure Machine Learning </h2>

| Unit  | Topic   |  Section      | Notes |
| :---    | :---   | :---   | :---   |
| Find the best classification model with Automated Machine Learning | [Introduction](https://learn.microsoft.com/en-gb/training/modules/find-best-classification-model-automated-machine-learning/1-introduction) | | - `AutoML`: try multiple preprocessing transformations and algorithms to find the best machine learning model. </br> - `AutoML` use UI of Azure ML studio, the Azure CLI, or the Python SDK.|
|| [Preprocess data and configure featurization](https://learn.microsoft.com/en-gb/training/modules/find-best-classification-model-automated-machine-learning/2-preprocess-data-configure-featurization)| - Understand scaling and normalization </br> - Configure optional featurization| - 1. create a data asset in Azure Machine Learning </br> - 2. create a MLTable data asset that includes the schema of the data </br> -  3. AutoML applies scaling and normalization to numeric data automatically </br> - Apply featurization, can be customized </br> -  get notified if AutoML has detected any issues with the data, like whether there are missing values or class imbalance.|
|| [Run an Automated Machine Learning experiment](https://learn.microsoft.com/en-gb/training/modules/find-best-classification-model-automated-machine-learning/3-run-job)| - The algorithms AutoML uses will depend on the task you specify. </br> - List of algorithms for classification </br> - You can choose to block individual algorithms from being selected (compliance or save time) </br> -  primary metric is the target performance metric for which the optimal model will be determined. </br> - Set the limits </br> - Set the training properties </br> - hen you use a compute cluster, you can have as many parallel trials as you have nodes </br> -  can exclude (or include) a subset of the available algorithms. </br> - an also choose whether you want to allow AutoML to use ensemble models. </br> - Code to submit and monitor|
||[Evaluate and compare models](https://learn.microsoft.com/en-gb/training/modules/find-best-classification-model-automated-machine-learning/4-evaluate-compare-models)| - data guardrails will automatically be applied too. list of guardrails for classification </br> - To explore a model even further, you can generate explanations for each model that has been trained. |
|| [Exercise - Find the best classification model with Automated Machine Learning](https://learn.microsoft.com/en-gb/training/modules/find-best-classification-model-automated-machine-learning/5-exercise)||
| Track model training in Jupyter notebooks with MLflow | [Configure MLflow for model tracking in notebooks](	https://learn.microsoft.com/en-gb/training/modules/track-model-training-jupyter-notebooks-mlflow/2-use-mlflow-model-tracking) | - MLflow is an open-source library for tracking and managing your machine learning experiments. </br> - MLflow Tracking is a component of MLflow that logs everything about the model you're training, such as parameters, metrics, and artifacts. </br> - Set up on Azure + locally | 
| | [Train and track models in notebooks](https://learn.microsoft.com/en-gb/training/modules/track-model-training-jupyter-notebooks-mlflow/3-train-models-notebooks) | - To group model training results, you'll use experiments. </br> -  Log results with MLflow </br> - Enable autologging: supports automatic logging for popular machine learning libraries </br> - When the job has completed, you can review all logged metrics in the studio. </br> - Use custom logging: log supplementary or custom material that isn't logged through autologging. </br> - Common functions used with custom logging are:|
|| [Exercise - Track model training](https://learn.microsoft.com/en-gb/training/modules/track-model-training-jupyter-notebooks-mlflow/4-exercise-track-model-training)||


<h2> 4. Optimize model training with Azure Machine Learning </h2>

| Unit  | Topic   | Comments       |
| :---    | :---   | :---     |
| Run a training script as a command job in Azure Machine Learning | [Convert a notebook to a script](https://learn.microsoft.com/en-gb/training/modules/run-training-script-command-job-azure-machine-learning/2-convert-notebook-script) | - recommendations when creating scripts to have production-ready code </br> - Remove nonessential code.</br> - Refactor your code into functions.</br> - Test your script in the terminal.</br> - How to navigate to the terminal? |
||[Run a script as a command job](https://learn.microsoft.com/en-gb/training/modules/run-training-script-command-job-azure-machine-learning/3-run-script-command-job)|- What parameters are needed to configure </br> - How the jobs are grupped|
||[Use parameters in a command job](https://learn.microsoft.com/en-gb/training/modules/run-training-script-command-job-azure-machine-learning/4-use-parameters-command-job)|- pass a parameter value to a script you want to run as a command job|
||[Exercise - Run a training script as a command job](https://learn.microsoft.com/en-gb/training/modules/run-training-script-command-job-azure-machine-learning/5-exercise-run-training-script-command-job)||
|Track model training with MLflow in jobs| [Introduction](https://learn.microsoft.com/en-gb/training/modules/train-models-training-mlflow-jobs/1-introduction)| - MLflow is an open-source platform that helps you to track model metrics and artifacts across platforms and is integrated with Azure Machine Learning </br> - can run training scripts locally or in the cloud and compare the results|
||[Track metrics with MLflow](https://learn.microsoft.com/en-gb/training/modules/train-models-training-mlflow-jobs/2-track-metrics-mlflow)|- Understand MLflow </br> -  two options to track machine learning jobs with MLflow: Before you can use either of these options, you need to set up the environment to use MLflow. </br> - Include MLflow in the environment</br> - Enable autologging</br> - Autologging logs parameters, metrics, and model artifacts without anyone needing to specify what needs to be logged.</br> - Log metrics with MLflow</br> - "Depending on the type of value you want to log, use the MLflow command to store the metric with the experiment run:"</br> - Submit the job|
| | [View metrics and evaluate models](https://learn.microsoft.com/en-gb/training/modules/train-models-training-mlflow-jobs/3-view-metrics-evaluate-models)| - View the metrics in the Azure Machine Learning studio</br> -  Retrieve runs and metrics with MLflow.</br> -  you can query the runs in a notebook by using MLflow</br> -  Retrieve runs (in a notebook)|
||[Exercise - Use MLflow to track training jobs](https://learn.microsoft.com/en-gb/training/modules/train-models-training-mlflow-jobs/4-exercise-use-mlflow-track-training-jobs)|- When a data scientist enables MLflow autologging, where can all model assets be found? |
|Perform hyperparameter tuning with Azure Machine Learning |[Introduction](https://learn.microsoft.com/en-gb/training/modules/perform-hyperparameter-tuning-azure-machine-learning-pipelines/1-introduction)|In Azure Machine Learning, you can tune hyperparameters by submitting a script as a sweep job.|
||[Define a search space](https://learn.microsoft.com/en-gb/training/modules/perform-hyperparameter-tuning-azure-machine-learning-pipelines/2-define-search-space)|- The set of hyperparameter values tried during hyperparameter tuning is known as the search space. </br> - How to define search space for descrete and continuous hyperparameters</br> - Defining a search space|
||[Configure a sampling method](https://learn.microsoft.com/en-gb/training/modules/perform-hyperparameter-tuning-azure-machine-learning-pipelines/3-configure-sampling-method)| - Grid sampling: Tries every possible combination.</br> -  Random sampling: Randomly chooses values from the search space</br> -  Sobol: Adds a seed to random sampling to make the results reproducible.</br> -  Bayesian sampling: Chooses new values based on previous results.</br> -  can only be applied when all hyperparameters are discrete</br> -  Sobol is a type of random sampling that allows you to use a seed. => reproducible</br> -  "You can only use Bayesian sampling with choice, uniform, and quniform parameter expressions.|
| |[Conrigure early termination](https://learn.microsoft.com/en-gb/training/modules/perform-hyperparameter-tuning-azure-machine-learning-pipelines/4-configure-early-termination)|- Can limit the number of trials</br> -  When to use an early termination policy depend on the search space and sampling method you're working with.</br> -  grid sampling method over a discrete search space => unnesessary most likely want to use an early termination policy when working with continuous hyperparameters and a random or Bayesian sampling method.</br> -  evaluation_interval: Specifies at which interval you want the policy to be evaluated. Every time the primary metric is logged for a trial counts as an interval.</br> -  delay_evaluation: Specifies when to start evaluating the policy. This parameter allows for at least a minimum of trials to complete without an early termination policy affecting them.</br> -  Two parameters to configure three options for early termination</br> -  Bandit policy: Uses a slack_factor (relative) or slack_amount(absolute). Any new model must perform within the slack range of the best performing model.</br> -  Median stopping policy: Uses the median of the averages of the primary metric. Any new model must perform better than the median.</br> - "Truncation selection policy: Uses a truncation_percentage, which is the percentage of lowest performing trials. Any new model must perform better than the lowest performing trials.</br> -  bandit policy to stop a trial if the target performance metric underperforms the best trial so far by a specified margin.</br> -  A median stopping policy abandons trials where the target performance metric is worse than the median of the running averages for all trials.</br> -  A truncation selection policy cancels the lowest performing X% of trials at each evaluation interval based on the truncation_percentage value you specify for X.|
||[Use a sweep job for hyperparameter tuning](https://learn.microsoft.com/en-gb/training/modules/perform-hyperparameter-tuning-azure-machine-learning-pipelines/5-use-sweep-job-hyperparameter-tuning)|- Create a training script for hyperparameter tuning</br> - whtat the script mast</br> - Configure and run a sweep job</br> - [Monitor and review sweep jobs](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters#visualize-hyperparameter-tuning-jobs?azure-portal=true)|
||[Exercise - Run a sweep job](https://learn.microsoft.com/en-gb/training/modules/perform-hyperparameter-tuning-azure-machine-learning-pipelines/6-exercise-run-sweep-job)||
| Run pipelines in Azure Machine Learning| [Introduction](https://learn.microsoft.com/en-gb/training/modules/run-pipelines-azure-machine-learning/1-introduction)||
||[Create components](https://learn.microsoft.com/en-gb/training/modules/run-pipelines-azure-machine-learning/2-create-components)|- two main reasons why you'd use components:</br> - create components when you're preparing your code for scale</br> - Components can be easily shared to other Azure Machine Learning users</br> - Create a component</br> - A component consists of three parts: load the component: To use components in a pipeline, you'll need the script and the YAML file</br> - Register the component: To make the components accessible to other users in the workspace, you can also register components to the Azure Machine Learning workspace|
||[Create a pipeline](https://learn.microsoft.com/en-gb/training/modules/run-pipelines-azure-machine-learning/3-create-pipeline)|- "a pipeline is a workflow of machine learning tasks in which each task is defined as a component.</br> - Each component can be run on a specific compute target, making it possible to combine different types of processing as required to achieve an overall goal.</br> - Build a pipeline|
||[Run a pipeline job](https://learn.microsoft.com/en-gb/training/modules/run-pipelines-azure-machine-learning/4-run-pipeline-job)|- Run a pipeline job</br> - Configure a pipeline job</br> - Troubleshoot a pipeline job</br> - Schedule a pipeline job</br> - A pipeline is ideal if you want to get your model ready for production. </br> - o automate the retraining of a model, you can schedule a pipeline.</br> - To delete a schedule, you first need to disable it:</br> - display names of the jobs triggered by the schedule|
||[Exercise - Run a pipeline job](https://learn.microsoft.com/en-gb/training/modules/run-pipelines-azure-machine-learning/5-exercise-run-pipeline-job)|- Incorrect. You need the RecurrenceTrigger class to create a schedule that runs at a regular interval.</br> - Correct. prep_data.outputs.output_data is the output of the step that prepares the data.|


<h2> 5. Manage and review models in Azure Machine Learning </h2>

| Unit  | Topic   | Comments       |
| :---    | :---   | :---     |
| Register an MLflow model in Azure Machine Learning   | [Log models with MLflow](https://learn.microsoft.com/en-gb/training/modules/register-mlflow-model-azure-machine-learning/2-log-mlflow-model)| - MLflow is an open source platform that streamlines machine learning deployment, regardless of the type of model you trained and the framework you used. </br> - Why use MLflow? </br> - MLflow standardizes the packaging of models, which means that an MLflow model can easily be imported or exported across different workflows.</br> - The MLmodel file contains the model's metadata, which allows for model traceability. </br> - MLflow allows you to log a model as an artifact, or as a model. </br> - [Difference between an artifcat and a model:](https://learn.microsoft.com/en-us/azure/machine-learning/concept-mlflow-models?view=azureml-api-2#the-mlmodel-format?azure-portal=true)</br> - Use autologging to log a model</br> - MLflow's autologging automatically logs parameters, metrics, artifacts, and the model you train.</br> - The framework you use to train your model is identified and included as the flavor of your model.</br> - Manually log a model</br> - The schemas of the expected inputs and outputs are defined as the signature in the MLmodel file. |
| | [Understand the MLflow model format](https://learn.microsoft.com/en-gb/training/modules/register-mlflow-model-azure-machine-learning/3-understand-mlflow-model-format)|- MLflow uses the MLModel format to store all relevant model assets in a folder or directory</br> -MLmodel file is the single source of truth about how the model should be loaded and used.</br> -A flavor is the machine learning library with which the model was created.</br> - Any MLflow python model can be loaded as a python_function mode</br> -There are two types of signatures:</br> -Column-based: used for tabular data with a pandas.Dataframe as inputs.</br> -Tensor-based: used for n-dimensional arrays or tensors (often used for unstructured data like text or images), with numpy.ndarray as inputs.</br> -the MLmodel file is created when you register the model|
||[Register an MLflow model](https://learn.microsoft.com/en-gb/training/modules/register-mlflow-model-azure-machine-learning/4-register-model)|- The model registry makes it easy to organize and keep track of your trained models.</br> - Registered models are identified by name and version</br> - There are three types of models you can register: </br> - Custom: Model type with a custom standard not currently supported by Azure Machine Learning.</br> - Triton: Model type for deep learning workloads. Commonly used for TensorFlow and PyTorch model deployments. </br> - MLflow: Model trained and tracked with MLflow. Recommended for standard use cases. </br> - To register an MLflow model, you can use the studio, the Azure CLI, or the Python SDK.|
||[Exercise - Log and register models with MLflow](https://learn.microsoft.com/en-gb/training/modules/register-mlflow-model-azure-machine-learning/5-exercise-register-mlflow-model)||
| Create and explore the Responsible AI dashboard for a model in Azure Machine Learning| [Indrocuction](https://learn.microsoft.com/en-gb/training/modules/manage-compare-models-azure-machine-learning/1-introduction)||
||[Understand Responsible AI](https://learn.microsoft.com/en-gb/training/modules/manage-compare-models-azure-machine-learning/2-understand-responsible-ai)| - Responsible Artificial Intelligence (Responsible AI) principles. </br> - Microsoft has listed five Responsible AI principles: </br> - Fairness and inclusiveness: Models should treat everyone fairly and avoid different treatment for similar groups. </br> - Reliability and safety: Models should be reliable, safe, and consistent. You want a model to operate as intended, handle unexpected situations well, and resist harmful manipulation. </br> - Privacy and security: Be transparent about data collection, use, and storage, to empower individuals with control over their data. Treat data with care to ensure an individual's privacy. </br> - Transparency: When models influence important decisions that affect people's lives, people need to understand how those decisions were made and how the model works. </br> - Accountability: Take accountability for decisions that models may influence and maintain human control.|
||[Create the Responsible AI dashboard](https://learn.microsoft.com/en-gb/training/modules/manage-compare-models-azure-machine-learning/3-create-responsible-dashboard)|- The Responsible AI dashboard allows you to pick and choose insights you need, to evaluate whether your model is safe, trustworthy, and ethical. </br> - To create a Responsible AI (RAI) dashboard, you need to create a pipeline by using the built-in components. </br> - Explore the Responsible AI components </br> - Add Explanation to RAI Insights dashboard: Interpret models by generating explanations. Explanations show how much features influence the prediction. </br> - Add Causal to RAI Insights dashboard: Use historical data to view the causal effects of features on outcomes. </br> - Add Counterfactuals to RAI Insights dashboard: Explore how a change in input would change the model's output. </br> - Add Error Analysis to RAI Insights dashboard: Explore the distribution of your data and identify erroneous subgroups of data. </br> - Build and run the pipeline to create the Responsible AI dashboard </br> - After you've trained and registered a model in the Azure Machine Learning workspace, you can create the Responsible AI </br> - "Using the Command Line Interface (CLI) extension for Azure Machine Learning. </br> - Using the Python Software Development Kit (SDK). </br> - Using the Azure Machine Learning studio for a no-code experience."|
|| [Evaluate the Responsible AI dashboard](https://learn.microsoft.com/en-gb/training/modules/manage-compare-models-azure-machine-learning/4-explore-responsible-dashboard)|- Depending on the components you selected, you can find the following insights in your Responsible AI dashboard:</br> - Error analysis</br> - Explanations</br> - Counterfactuals</br> - Causal analysis</br> - Error tree map: Allows you to explore which combination of subgroups results in the model making more false predictions.</br> - Error heat map: Presents a grid overview of a model's errors over the scale of one or two features.</br> - There are various statistical techniques you can use as model explainers. Most commonly, the mimic explainer trains a simple interpretable model on the same data and task. As a result, you can explore two types of feature importance:</br> - Aggregate feature importance: Shows how each feature in the test data influences the model's predictions overall.</br> - "Individual feature importance: Shows how each feature impacts an individual prediction.</br> - To explore how the model's output would change based on a change in the input, you can use counterfactuals.</br> - Causal analysis uses statistical techniques to estimate the average effect of a feature on a desired prediction. It analyzes how certain interventions or treatments may result in a better outcome, across a population or for a specific individual.</br> - There are three available tabs in the Responsible AI dashboard when including causal analysis:</br> - Aggregate causal effects: Shows the average causal effects for predefined treatment features (the features you want to change to optimize the model's predictions).</br> - Individual causal effects: Shows individual data points and allows you to change the treatment features to explore their influence on the prediction.</br> - reatment policy: Shows which parts of your data points benefit most from a treatment.|
||[Exercise - Explore the Responsible AI dashboard](https://learn.microsoft.com/en-gb/training/modules/manage-compare-models-azure-machine-learning/5-exercise-explore-responsible-dashboard)|


<h2> 6. Deploy and consume models with Azure Machine Learning </h2>

| Unit  | Topic   | Comments       |
| :---    | :---   | :---     |
| Deploy a model to a managed online endpoint| [Intro](https://learn.microsoft.com/en-gb/training/modules/deploy-model-managed-online-endpoint/1-introduction) | - To consume the model, you need to deploy it </br> - One way to deploy a model is to integrate it with a service that allows applications to request instant, or real-time, predictions for individual or small sets of data points|
| | [Explore managed online endpoints](https://learn.microsoft.com/en-gb/training/modules/deploy-model-managed-online-endpoint/2-explore-managed-online-endpoints)| - Real-time predictions </br> - An endpoint is an HTTPS endpoint to which you can send data, and which will return a response (almost) immediately. </br> - Azure Machine Learning, there are two types of online endpoints </br> - Managed online endpoints: Azure Machine Learning manages all the underlying infrastructure. </br> - Kubernetes online endpoints: Users manage the Kubernetes cluster which provides the necessary infrastructure. </br> - a managed online endpoint, you only need to specify the virtual machine (VM) type and scaling setting </br> - Deploy your model 4 things to specify to deploy your model to a managed online endpoint </br> - Blue/green deployment </br> - One endpoint can have multiple deployments. </br> - 90% of the traffic can go to the blue deployment*, and 10% of the traffic can go to the green deployment </br> - Create an endpoint|
||[Deploy your MLflow model to a managed online endpoint](https://learn.microsoft.com/en-gb/training/modules/deploy-model-managed-online-endpoint/3-deploy-your-mlflow-model-managed-online-endpoint)| - The easiest way to deploy a model to an online endpoint is to use an MLflow model and deploy it to a managed online endpoint. </br> - Deploy an MLflow model to an endpoint </br> - "don´t need to have the scoring script and environment." </br> - must have model files stored on a local path or with a registered model </br> - you also need to specify the compute configuration for the deployment|
|| [Deploy a model to a managed online endpoint](https://learn.microsoft.com/en-gb/training/modules/deploy-model-managed-online-endpoint/4-eploy-custom-model-managed-online-endpoint)|- "need to create the scoring script and define the environment necessary during inferencing.</br> - Deploy a model to an endpoint </br> - Pre-requisits</br> - Create the scoring script</br> - Create an environment</br> - Create the deployment</br> - can specify the compute configuration with two parameters (instance type, instance count)|
||[Test managed online endpoints](https://learn.microsoft.com/en-gb/training/modules/deploy-model-managed-online-endpoint/5-monitor-online-endpoints)| - Use the Azure Machine Learning studio</br> - List all endpointx</br> - Use the Azure Machine Learning Python SDK</br> - The response from the deployed model is a JSON collection with a prediction for each case that was submitted in the data.|
|| [Exercise - Deploy an MLflow model to an online endpoint](https://learn.microsoft.com/en-gb/training/modules/deploy-model-managed-online-endpoint/6-exercise-deploy-mlflow-model-online-endpoint)| - You should use Kubernetes online endpoint if you want to manage the underlying Kubernetes clusters.|
|Deploy a model to a batch endpoint| [Introduction](https://learn.microsoft.com/en-gb/training/modules/deploy-model-batch-endpoint/1-introduction) |- in machine learning, batch inferencing is used to asynchronously apply a predictive model to multiple cases and write the results to a file or database.|
|| [Understand and create batch endpoints](https://learn.microsoft.com/en-gb/training/modules/deploy-model-batch-endpoint/2-explore-batch-endpoints)|- Batch predictions</br> - An endpoint is an HTTPS endpoint that you can call to trigger a batch scoring job.</br> - can trigger the batch scoring job from another service, such as Azure Synapse Analytics or Azure Databricks.  => can integrate with existing pipeline</br> - uses a compute cluster to score multiple inputs</br> - Create a batch endpoint</br> - Use compute clusters for batch deployments</br> - The ideal compute to use for batch deployments is the Azure Machine Learning compute cluster|
|| [Deploy your MLflow model to a batch endpoint](https://learn.microsoft.com/en-gb/training/modules/deploy-model-batch-endpoint/3-deploy-your-mlflow-model-batch-endpoint)| - Register an MLflow model </br> - To avoid needed a scoring script and environment, an MLflow model needs to be registered in the Azure Machine Learning workspace </br> - Deploy an MLflow model to an endpoint</br> - need to specify how you want the batch scoring job to behave|
||[Deploy a custom model to a batch endpoint](https://learn.microsoft.com/en-gb/training/modules/deploy-model-batch-endpoint/4-deploy-custom-model-batch-endpoint)| - Create the scoring script: that reads the new data, loads the model, and performs the scoring. </br> - must include two functions: init(), run() </br> - Create an environment: can create an environment with a Docker image with Conda dependencies, or with a Dockerfile. Configure and create the deployment|
|| [Invoke and troubleshoot endpoints](https://learn.microsoft.com/en-gb/training/modules/deploy-model-batch-endpoint/5-monitor-batch-endpoints)| - When you invoke a batch endpoint, you trigger an Azure Machine Learning pipeline job.</br> - Trigger the batch scoring job </br> - Troubleshoot a batch scoring job|
|| [Exercise - Deploy an MLflow model to a batch endpoint](https://learn.microsoft.com/en-gb/training/modules/deploy-model-batch-endpoint/6-exercise-deploy-mlflow-model-batch-endpoint)| - The default deployment will be used to do the actual batch scoring when the endpoint is invoked.|
